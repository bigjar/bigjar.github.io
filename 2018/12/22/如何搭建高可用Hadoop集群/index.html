<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="如何搭建高可用Hadoop集群"><meta name="keywords" content="Hadoop"><meta name="author" content="MrHook"><meta name="copyright" content="MrHook"><title>如何搭建高可用Hadoop集群 | MrHook的时光机</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.7.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.7.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script><meta name="generator" content="Hexo 4.2.0"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#介绍"><span class="toc-number">1.</span> <span class="toc-text">介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NameNode可用性"><span class="toc-number">2.</span> <span class="toc-text">NameNode可用性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS高可用架构"><span class="toc-number">3.</span> <span class="toc-text">HDFS高可用架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HA架构的实现"><span class="toc-number">4.</span> <span class="toc-text">HA架构的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Quorum-Journal-Nodes"><span class="toc-number">4.1.</span> <span class="toc-text">Quorum Journal Nodes</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#NameNode隔离"><span class="toc-number">4.1.1.</span> <span class="toc-text">NameNode隔离</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#使用共享存储"><span class="toc-number">4.2.</span> <span class="toc-text">使用共享存储</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#自动失败恢复"><span class="toc-number">4.3.</span> <span class="toc-text">自动失败恢复</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#搭建和配置Hadoop的高可用集群"><span class="toc-number">4.4.</span> <span class="toc-text">搭建和配置Hadoop的高可用集群</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考"><span class="toc-number">5.</span> <span class="toc-text">参考</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">MrHook</div><div class="author-info__description text-center">stay hungry, stay foolish</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">38</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">28</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">6</span></a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">MrHook的时光机</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">如何搭建高可用Hadoop集群</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-12-22</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>在这篇文章中，我将讲述HDFS 2.x高可用集群架构和如何搭建一个高可用的HDFS集群。其内容如下：</p>
<ul>
<li><p>HDFS HA架构</p>
<ul>
<li>介绍</li>
<li>NameNode高可用</li>
<li>HA架构</li>
<li>HA的实现（JournalNode和共享存储）</li>
</ul>
</li>
<li><p>如何设置Hadoop集群的HA(Quorum Journal Nodes)？</p>
</li>
</ul>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>为了解决Hadoop1.x的单点故障问题，Hadoop2.x引入了高可用集群的概念。HDFS架构遵从Master/Slave拓扑，NameNode作为Master负责管理Slave节点(DataNode)。由于Master只有一个节点，因此它将成为整个系统的瓶颈。尽管引入的Secondary NameNode降低了NameNode中数据丢失的风险，但是它并没有解决NameNode高可用的问题。</p>
<h2 id="NameNode可用性"><a href="#NameNode可用性" class="headerlink" title="NameNode可用性"></a>NameNode可用性</h2><p>如果使用HDFS的默认配置，NameNode会有单点故障的风险。当NameNode不可用的时候，整个系统将会是不可用的，直到NameNode重启或者引入新的NameNode。</p>
<p>NameNode的不可用的原因可能是以下几个方面：</p>
<ul>
<li>软件或者硬件的升级</li>
<li>NameNode的崩溃</li>
</ul>
<h2 id="HDFS高可用架构"><a href="#HDFS高可用架构" class="headerlink" title="HDFS高可用架构"></a>HDFS高可用架构</h2><p>HA架构通过设置两个NameNode(一主一备)来解决NameNode的可用性问题。因此，在一个高可用的集群中，将同时会有两个NameNode在运行。</p>
<ul>
<li>Active NameNode</li>
<li>Standby/Passive NameNode</li>
</ul>
<p><img src="HDFS-HA-Architecture.png" alt="HDFS-HA-Architecture"></p>
<p>如果一个NameNode挂了，另一个NameNode将会接管整个集群，从而减少集群的下线时间。备用的NameNode拥有集群的失败保护能力。因此当主节点挂了之后，通过被节点我们能够自动失败恢复。</p>
<p>但是，在保持HDFS高可用集群的一致性上，将会有以下两个问题：</p>
<ul>
<li>主备节点相互同步。拥有相同的元数据，可以使得主节点挂了之后，备节点可以快速的失败恢复。</li>
<li>同时只能有一个主节点。当有两个主节点时，将会导致数据竞争。一个集群被分裂成两个小集群，这种场景被称为脑裂。为了避免这种场景的发生，必须建立一个栅栏来确保同时只有一个主节点。</li>
</ul>
<h2 id="HA架构的实现"><a href="#HA架构的实现" class="headerlink" title="HA架构的实现"></a>HA架构的实现</h2><p>在HDFS高可用架构中，同时运行有两个NameNode，一个主，一个备。它通过下面两种方式来实现主备节点配置。</p>
<ul>
<li>使用Quorum Journal Nodes</li>
<li>使用NFS共享存储</li>
</ul>
<h3 id="Quorum-Journal-Nodes"><a href="#Quorum-Journal-Nodes" class="headerlink" title="Quorum Journal Nodes"></a>Quorum Journal Nodes</h3><p><img src="JournalNode-HDFS-HA-Architecture.png" alt="JournalNode-HDFS-HA-Architecture"></p>
<ul>
<li>主备节点通过一组独立的叫做JournalNodes的节点来保持数据同步。JournalNodes之间相互连接，然后把收到的请求信息复制到其他节点。</li>
<li>主NameNode负责更新JournalNodes中的EditLogs（元信息）。</li>
<li>备NameNode读取JournalNode中EditLogs中的变化，并且应用到它自己的命名空间。</li>
<li>在失败恢复期间，备NameNode需要确保在称为主节点之间更新JournalNodes中的元信息。这使得当前的命名空间与失败之前的命名空间状态同步。</li>
<li>两个NameNode的IP地址对于所有的DataNodes都是可见的。同时DataNodes发送心跳包和阻塞位置信息给两个NameNode。这样，由于备节点及时更新了集群的额阻塞信息，从而可以更快的恢复。</li>
</ul>
<h4 id="NameNode隔离"><a href="#NameNode隔离" class="headerlink" title="NameNode隔离"></a>NameNode隔离</h4><p>之前讨论过，同时只有一个主NameNode对整个集群来说是非常重要的。因此，NameNode之间的隔离就显得尤为重要。</p>
<ul>
<li>JournalNodes只允许同时只有一个NameNode可以写日志。</li>
<li>主节点挂之后，备节点负责写日志到JournalNodes，并且禁止其他NameNode再成为主节点。</li>
<li>最后，备节点就转换为主节点。</li>
</ul>
<h3 id="使用共享存储"><a href="#使用共享存储" class="headerlink" title="使用共享存储"></a>使用共享存储</h3><p><img src="Shared-Storage-HDFS-HA-Architecture.png" alt="Shared-Storage-HDFS-HA-Architecture"></p>
<ul>
<li>主备NameNode节点通过共享存储来保持同步。主NameNode把它在自己的命名空间中修改的日志打到共享存储的EditLog中。备节点读取共享存储中的EditLog，并更新到自己的命名空间中。</li>
<li>在失败恢复的时候，备节点首先使用共享存储中的EditLog来更新元信息。然后，它就成为了主节点。这样，就使得当前的命名空间的状态跟失败恢复之前的状态同步。</li>
<li>管理员必须配置至少一个栅栏方法来防止脑裂场景。</li>
<li>系统可以使用很多栅栏隔离机制。比如杀死NameNode的进程和废除进入共享存储目录的权限。</li>
<li>另外，我们可以使用STONITH技术来隔离先前的主节点。STONOTH使用专门的分布式单元来强制下线主NameNode机器。</li>
</ul>
<h3 id="自动失败恢复"><a href="#自动失败恢复" class="headerlink" title="自动失败恢复"></a>自动失败恢复</h3><p>失败恢复是指一种当检测到失败时，自动转换到次要系统。总共有两种失败恢复：</p>
<ul>
<li>优雅的失败恢复：手动初始化失败恢复。</li>
<li>自动失败恢复：当NameNode挂了之后，自动初始化失败恢复程序。</li>
</ul>
<p>Apache Zookeeper为HDFS高可用集群提供自动失败恢复的能力。它保持非常小的协调数据，通知客户端数据改变和监测客户端的失败。Zookeeper保持与NameNode的会话。在失败的情况下，会话将会过期，Zookeeper将通知其他NameNode初始化失败恢复程序。同时，其他备NameNode将会锁住Zookeeper，来使得自己成为主NameNode。</p>
<p>ZookeerFailoverController (ZKFC) 是一个Zookeeper客户端，能够监测和管理NameNode的状态。每个NameNode运行一个ZKFC。ZKFC负责周期性的监测NameNodes的健康状态。</p>
<p>现在我们了解了高可用的Hadoop集群，接下来就开始部署集群。</p>
<p>在主NameNode运行的后台程序有：</p>
<ul>
<li>Zookeeper Fail Over controller</li>
<li>JournalNode</li>
<li>NameNode</li>
</ul>
<p>备NamdeNode运行的后台程序有：</p>
<ul>
<li>Zookeeper Fail Over controller</li>
<li>JournalNode</li>
<li>NameNode</li>
</ul>
<p>DataNode上运行的后台程序有：</p>
<ul>
<li>JournalNode</li>
<li>DataNode</li>
</ul>
<h3 id="搭建和配置Hadoop的高可用集群"><a href="#搭建和配置Hadoop的高可用集群" class="headerlink" title="搭建和配置Hadoop的高可用集群"></a>搭建和配置Hadoop的高可用集群</h3><p>1.每个节点设置JAVA环境和host。</p>
<table>
<thead>
<tr>
<th>虚拟机</th>
<th>IP地址</th>
<th>主机名</th>
</tr>
</thead>
<tbody><tr>
<td>主NameNode</td>
<td>192.168.60.1</td>
<td>hadoop01</td>
</tr>
<tr>
<td>备NameNode</td>
<td>192.168.60.2</td>
<td>hadoop02</td>
</tr>
<tr>
<td>DataNode</td>
<td>192.168.60.3</td>
<td>hadoop03</td>
</tr>
</tbody></table>
<p>2.每个节点设置ssh免密登录。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ssh-keygen -t rsa</span></span><br></pre></td></tr></table></figure>

<p>一直按enter直到结束。当都生成ssh密钥后，你会得到一个公钥和私钥。</p>
<p><strong>注意：.ssh目录的权限需要设置为700，.ssh目录里面文件的权设置为600。</strong></p>
<p>然后把每台机器上的id_rsa.pub内容复制到authrozied_keys文件中。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ssh-copy-id –i .ssh/id_rsa.pub root@hadoop02</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ssh-copy-id –i .ssh/id_rsa.pub root@hadoop03</span></span><br></pre></td></tr></table></figure>

<p>接着重启所有节点的ssh服务。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> service sshd restart</span></span><br></pre></td></tr></table></figure>

<p>接着你可以通过ssh命令免密登录到其他机器上。如果还是需要密码，请检查以上设置是否正确。</p>
<p>3.下载和解压Hadoop和Zookeeper的二进制文件。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> wget https://archive.apache.org/dist/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> wget https://archive.apache.org/dist/hadoop/core/hadoop-2.6.0/hadoop-2.6.0.tar.gz</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> tar –xvf zookeeper-3.4.6.tar.gz</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> tar –xvf hadoop-2.6.0.tar.gz</span></span><br></pre></td></tr></table></figure>

<p>4设置hadoop环境。打开.bashrc文件。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vi ~/.bashrc</span></span><br></pre></td></tr></table></figure>

<p>添加以下内容：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=&lt; Path to your Hadoop-2.6.0 directory&gt;</span><br><span class="line">export HADOOP_MAPRED_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_COMMON_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_HDFS_HOME=$HADOOP_HOME</span><br><span class="line">export YARN_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br><span class="line">export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br><span class="line">export JAVA_HOME=&lt;Path to your Java Directory&gt;</span><br><span class="line">export ZOOKEEPER_HOME =&lt;Path to your Zookeeper Directory&gt;</span><br><span class="line">export PATH=$PATH: $JAVA_HOME/bin: $HADOOP_HOME/bin: $HADOOP_HOME/sbin:$ZOOKEEPER_HOME/bin</span><br></pre></td></tr></table></figure>

<p>5.设置hadoop配置文件</p>
<p>编辑core-site.xml文件，增加以下内容。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>Athena<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://ha-cluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	</span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/HA/jn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>编辑hdfs-site.xml文件，增加以下内容</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/HA/namenode<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span>ha-cluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.ha-cluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ha-cluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop01:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ha-cluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ha-cluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop01:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ha-cluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://hadoop01:8485;hadoop02:8485;hadoop03:8485/ha-cluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.ha-cluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span> hadoop01:2181,hadoop02:2181,hadoop03:2181 <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span>sshfence<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/root/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/HA/app/name-ha<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/HA/app/tmp-ha<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>6.设置zookeeper配置文件</p>
<p>编辑zoo.cfg，增加以下内容</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Server.1=hadoop01:2888:3888</span><br><span class="line">Server.2=hadoop02:2888:3888</span><br><span class="line">Server.3=hadoop03:2888:3888</span><br></pre></td></tr></table></figure>

<p>7.拷贝文件到各个节点</p>
<p>完成上述步骤后，把上面设置过的.bashrc文件，hadoop目录，zookeeper目录通过scp命令复制到其他节点。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> scp –r &lt;path of directory&gt; edureka@&lt;ip address&gt;:&lt;path <span class="built_in">where</span> you need to copy&gt;</span></span><br></pre></td></tr></table></figure>

<p>8.为zookeeper节点设置id。</p>
<p>在zookeeper的配置文件目录中，增加一个myid的文件。然后每个节点分别设置为1，2，3。</p>
<p>9.启动hadoop集群</p>
<p>在配置了journalnode的节点上启动journalnode。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop-daemon.sh start journalnode</span><br></pre></td></tr></table></figure>

<p>在主NameNode节点上格式化HDFS，并启动NameNode。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ HDFS namenode -format</span><br><span class="line">$ hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure>

<p>复制HDFS元信息到备NameNode，并启动备NameNode。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ HDFS namenode -bootstrapStandby</span><br><span class="line">$ hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure>

<p>在每个节点上启动zookeeper。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ zkServer.sh start</span><br></pre></td></tr></table></figure>

<p>在启动zookeeper服务后，你在每个节点上通过JPS命令可以看到QuorumPeerMain进程。</p>
<p>启动DataNode。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop-daemon.sh start datanode</span><br></pre></td></tr></table></figure>

<p>格式化zookeeper数据。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ HDFS zkfc –formatZK</span><br></pre></td></tr></table></figure>

<p>在主备NameNode启动启动ZKFC服务。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop-daemon.sh start zkfc</span><br></pre></td></tr></table></figure>

<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.edureka.co/blog/how-to-set-up-hadoop-cluster-with-hdfs-high-availability/" target="_blank" rel="noopener">How to Set Up Hadoop Cluster with HDFS High Availability</a></p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">MrHook</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://bigjar.github.io/2018/12/22/%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8Hadoop%E9%9B%86%E7%BE%A4/">https://bigjar.github.io/2018/12/22/%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8Hadoop%E9%9B%86%E7%BE%A4/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Hadoop/">Hadoop</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2019/11/08/%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97Flink%E9%9B%86%E7%BE%A4%E8%A7%84%E6%A8%A1%EF%BC%9A%E4%BF%A1%E5%B0%81%E8%83%8C%E8%AE%A1%E7%AE%97%E6%B3%95/"><i class="fa fa-chevron-left">  </i><span>如何计算Flink集群规模：信封背计算法</span></a></div><div class="next-post pull-right"><a href="/2018/11/13/Akka%E5%9C%A8Flink%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/"><span>Akka在Flink中的应用</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2020 By MrHook</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.7.0"></script><script src="/js/fancybox.js?version=1.7.0"></script><script src="/js/sidebar.js?version=1.7.0"></script><script src="/js/copy.js?version=1.7.0"></script><script src="/js/fireworks.js?version=1.7.0"></script><script src="/js/transition.js?version=1.7.0"></script><script src="/js/scroll.js?version=1.7.0"></script><script src="/js/head.js?version=1.7.0"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>