<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>MrHup.code</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="MrHup.code">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="MrHup.code">
<meta property="og:locale" content="cn">
<meta property="article:author" content="MrHup">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="MrHup.code" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">MrHup.code</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">HeapAndStack</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-深入理解Flink网络栈及背压监控" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/02/20/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Flink%E7%BD%91%E7%BB%9C%E6%A0%88%E5%8F%8A%E8%83%8C%E5%8E%8B%E7%9B%91%E6%8E%A7/" class="article-date">
  <time datetime="2020-02-20T06:51:26.000Z" itemprop="datePublished">2020-02-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/02/20/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Flink%E7%BD%91%E7%BB%9C%E6%A0%88%E5%8F%8A%E8%83%8C%E5%8E%8B%E7%9B%91%E6%8E%A7/">深入理解Flink网络栈及背压监控</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Flink的网络栈是flink-runtime中非常核心的模块。TaskManagers的subtask通过它相互相互连接。与用于协调TaskManager和JobManager之间通信的Akka(RPC)不同，它使用的是底层API，Netty。它对Flink任务的吞吐量和延迟性能至关重要。因此，监控它的状态来查看当前的任务瓶颈也是非常必要的。目前有两种监控方式:背压监控和网络监控。</p>
<h2 id="逻辑图"><a href="#逻辑图" class="headerlink" title="逻辑图"></a>逻辑图</h2><p>当执行keyby()算子时，Flink会提供subtaks之间相互通信的逻辑图，如下所示：<br><img src="flink-network-stack1.png" alt="逻辑图"><br>它抽象了三个概念的设置：</p>
<ul>
<li>subtask的输出类型<ul>
<li>pipelined(有界 or 无界)：尽可能快的one by one的向下游发送有界或无界的数据。</li>
<li>blocking：只有结果全部生成才往下游发送数据。</li>
</ul>
</li>
<li>调度类型<ul>
<li>all at once(迫切的)：同时部署任务的所有子任务(流应用)。</li>
<li>next stage on first output(懒惰的)：只有上游有数据产生，就部署下游任务。</li>
<li>next stage on complete output：上游结果全部生成，才部署下游任务。</li>
</ul>
</li>
<li>传输<ul>
<li>高吞吐：Flink缓存一批数据到网络缓存(network buffer)中，在存满或者超时批次发送（不是一条条的向下游发送）。通过降低每条数据的花销来提高吞吐量。</li>
<li>通过缓存超时的低延迟：通过降低缓存的超时时间，可以牺牲吞吐量而换取低延迟。</li>
</ul>
</li>
</ul>
<p><img src="table1.png" alt=""></p>
<p>此外，对于subtask有多个输入的情况，有两种调度方式：所有的输入都完成或者其中一个输入完成一个或全部的结果。对于批作业的调度模型选择，可以设置<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/api/java/org/apache/flink/api/common/ExecutionConfig.html#setExecutionMode-org.apache.flink.api.common.ExecutionMode-" target="_blank" rel="noopener">ExecutionConfig#setExecutionMode()</a>、<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/api/java/org/apache/flink/api/common/ExecutionMode.html#enum.constant.detail" target="_blank" rel="noopener">ExecutionMode</a> 、<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/api/java/org/apache/flink/api/common/ExecutionConfig.html#setDefaultInputDependencyConstraint-org.apache.flink.api.common.InputDependencyConstraint-" target="_blank" rel="noopener">ExecutionConfig#setDefaultInputDependencyConstraint()</a>。</p>
<h2 id="物理传输"><a href="#物理传输" class="headerlink" title="物理传输"></a>物理传输</h2><p>假定2个taskmanger，每个taskmanager提供2个slot，并行度为4。TaskManger1执行subtask A.1，A.2，B.1和B.2，TaskManager2执行subtaskA.3，A.4，B.3和B.4。在taskA和B有shuffle的情况下，比如keyBy()，每个TaskManager就会有2X4的逻辑连接。</p>
<p><img src="table2.png" alt=""></p>
<p>不同task之间的远程网络连接都有自己的TCP通道。但是在同一个TaskManager中的的subtask共享同一个TCP通道，这样可以降低资源消耗。<br>每个task的结果叫做 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/api/java/org/apache/flink/runtime/io/network/partition/ResultPartition.html" target="_blank" rel="noopener">ResultPartition</a>，根据subtask又可以分成<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/api/java/org/apache/flink/runtime/io/network/partition/ResultSubpartition.html" target="_blank" rel="noopener">ResultSubpartitions</a> - 对应</p>
<p><img src="flink-network-stack2.png" alt=""></p>
<p>一个逻辑通道。在网络栈中，Flink不再处理单独的数据，而是封装在网络缓存(network buffer)中的一组序列化的数据。每个subtask的缓存从自己的本地缓存池中拿取，计算公式如下：<br>$$<br>channels * buffersPerChannel + floatingBuffersPerGate<br>$$<br>通常TaskManager的总缓存数不需要设置。如果要设置，可以参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/config.html#configuring-the-network-buffers" target="_blank" rel="noopener">Configuring the 网络缓存(network buffer)s</a> 。</p>
<h3 id="背压问题"><a href="#背压问题" class="headerlink" title="背压问题"></a>背压问题</h3><p>在发送端，当一个subtask的发送缓存池被耗尽时-(缓存数据要么被暂存在结果子分区(result subpartiiton)的缓存队列中，要么在Netty网络栈中)，那么生产者就会被block住，不能继续发送数据，这就是背压产生的原因。接收端以类似的方式工作：任何进来的Netty缓存需要先通过网络缓存(network buffer)。如果在缓存池中没有可用的网络缓存(network buffer)，Flink将会停止读取读取通道数据直到有可用的缓存。这会极大的影响同一个TaskManager之下的多个subtask的共享通道。下图展示了subtask B.4因为没有缓存引起了背压，从而导致B.3在缓存池中还有可用缓存的情况下依然不能接收和处理数据。</p>
<p><img src="flink-network-stack3.png" alt=""></p>
<p>为了阻止这个问题，Flink1.5引入了流控机制。</p>
<h2 id="基于信誉的流控"><a href="#基于信誉的流控" class="headerlink" title="基于信誉的流控"></a>基于信誉的流控</h2><p>基于信誉的流控确保了任何subtask都有接收数据的能力。它是对可用网络缓存(network buffer)机制的扩展。subtask不再仅有一个本地缓存池，每个远程的输入通道都有自己的exclusive buffer。同时，在本地缓存池中的缓存叫做floating buffers，因为它们是浮动的，对每一个输入通道都可用。<br>接收端将会向发送端发送一个可用的缓存数作为credits(1 buffer = 1 credit)。每个结果子分区(result subpartiiton)将会持续追踪它的通道信誉。缓存会向底层网络栈（netty）中前进。在缓存之外，发送端还会发送当前的backlog大小，它表示当前有多少缓存等待在子分区队列中。接收端将获取适当的floating buffer来加快处理backlog。只要有backlog，接收端将会尽可能多的获取floating buffer，但是有时缓存可能被用完。接收端将利用取得的缓存并持续监听可用的缓存。<br><img src="flink-network-stack4.png" alt=""><br>基于信誉的流控使用 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/config.html#taskmanager-network-memory-buffers-per-channel" target="_blank" rel="noopener">buffers-per-channel</a>来设置有exclusive缓存的数量，使用 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/config.html#taskmanager-network-memory-floating-buffers-per-gate" target="_blank" rel="noopener">floating-buffers-per-gate</a>来设置本地缓存池。这两个参数的默认值是被设置为最大吞吐量的值，也就是没有流控。可以根据网络往返时间和带宽来动态调整这两个值。<br>如果没有可用的缓存可用，每个缓存池会获取全局可用的缓存。</p>
<h3 id="背压问题-1"><a href="#背压问题-1" class="headerlink" title="背压问题"></a>背压问题</h3><p>与没有流控的背压机制相反，信誉提供更加直接的控制：如果接收端无法跟上，它的可用信誉将最终减为0，并停止向底层网络栈(netty)发送缓存。这样只会背压这个逻辑通道，而不需要block整个多路复用的TCP通道。因此，其他接收端不会收到影响。</p>
<h3 id="流控的影响"><a href="#流控的影响" class="headerlink" title="流控的影响"></a>流控的影响</h3><p>因为流控，多路复用通道不再block其他的逻辑通道，整个资源利用率将会增加。此外，因为能够知道有多少数据被背压，还能够加强 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/internals/stream_checkpointing.html#checkpointing" target="_blank" rel="noopener">checkpoint alignments</a>：如果没有流控，TCP通道填充和传播网络堆栈的内部缓冲区，需要一定的时间耗费，此时接收方不再读取的消息。在那期间，会出现很多缓存等待的情况。排在这些等待缓存后面的checkpoint barrier需要等到这些缓存被处理掉，它才能继续前进，这样就增加了checkpoint对齐的时间。<br><img src="flink-network-stack5.png" alt=""><br>但是，接收端额外的消息声明会引起额外的一些损耗，特别是在SSL加密的通道设置中。另外因为exclusive buffers不是共享的，单个输入通道不能使用缓存池中的所有缓存。同样，发送端无法发送尽可能多的数据(不能超过返回的信誉值)，这样可能需要耗费更多的时间来发送数据。虽然这会影响任务的性能，但是通常使用流控会有更好的优势。虽然增加的exclusive buffers会耗费更多的内存，但是整体内存使用会更低，因为底层网络栈(netty)不再需要缓存更多的数据。<br>使用流控的另外一个好处是：因为在接收端和发送端缓存更少的数据，从而会更早的发生背压。如果想要缓存更多数据的话，可以通过设置 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/config.html#taskmanager-network-memory-floating-buffers-per-gate" target="_blank" rel="noopener">floating-buffers-per-gate</a>来增加floating buffer的数量。<br><img src="table3.png" alt=""><br>可以通过taskmanager.network.credit-model: false来关闭流控。不过这个选项将在以后的版本中移除。</p>
<h2 id="Network-Buffer数据的读写"><a href="#Network-Buffer数据的读写" class="headerlink" title="Network Buffer数据的读写"></a>Network Buffer数据的读写</h2><p>下图展示了简化的网络栈视图与它的组件。<br><img src="flink-network-stack6.png" alt=""><br><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/api/java/org/apache/flink/runtime/io/network/api/writer/RecordWriter.html" target="_blank" rel="noopener">RecordWriter</a>把java对象序列化，并放入网络缓存(network buffer)中。RecordWriter首先使用<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/api/java/org/apache/flink/runtime/io/network/api/serialization/SpanningRecordSerializer.html" target="_blank" rel="noopener">SpanningRecordSerializer</a>序列化数据到一个可扩展的堆字节数组中。然后，它尝试把这些字节数组写入目标网络通道中的网络缓存(network buffer)中。<br>在接收端，底层网络栈把收到的缓存写入合适的输入通道中。task线程最终会读取这些缓存队列并通过<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/api/java/org/apache/flink/runtime/io/network/api/reader/RecordReader.html" target="_blank" rel="noopener">RecordReader</a>的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/api/java/org/apache/flink/runtime/io/network/api/serialization/SpillingAdaptiveSpanningRecordDeserializer.html" target="_blank" rel="noopener">SpillingAdaptiveSpanningRecordDeserializer</a>尝试把这些反序列化成java对象。与序列化类似，反序列化器也必须处理特殊的情况，比如因为数据太大(默认32kb，通过<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/config.html#taskmanager-memory-segment-size" target="_blank" rel="noopener">taskmanager.memory.segment-size</a>设置)分散在多个网络缓存(network buffer)中，或者因为网络缓存(network buffer)没有过多剩余的buffer导致数据被打散。</p>
<h3 id="往Netty中刷新缓存"><a href="#往Netty中刷新缓存" class="headerlink" title="往Netty中刷新缓存"></a>往Netty中刷新缓存</h3><p>在上图中，基于信誉的流控机制实际是在”Netty Server”组件中，RecordWriter写的缓存会被添加到结果子分区(result subpartiiton)。但是Netty什么时候开始获取缓存呢？显然，它不能随意的去刷新字节，因为这不仅将增加跨线程通信和同步的花费，还会使得整个缓存过期。</p>
<p>在Flink中，有三种情况Netty server可以获取可用的缓存：</p>
<ul>
<li>当缓存被数据写满</li>
<li>缓存超时触发</li>
<li>特殊事件，比如checkpoint barrier</li>
</ul>
<h2 id="网络监控"><a href="#网络监控" class="headerlink" title="网络监控"></a>网络监控</h2><p>网络监控中主要的就是 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/monitoring/back_pressure.html" target="_blank" rel="noopener">monitoring backpressure</a>。当系统收到的吞吐量高于能处理的吞吐量时，就会导致背压，这主要有两种原因：</p>
<ul>
<li><p>接收端处理慢。有可能是因为接收端自己也是背压，作为发送端无法持续发送数据。或者因为垃圾回收，缺乏系统资源比如I/O等。</p>
</li>
<li><p>网络通道慢。在这种情况下，接收端没有直接涉及。背压产生由于发送端过度消耗机器的共享网络带宽。因为其他组件也需要耗费网络资源，比如sources和sinks，分布式文件系统(checkpointing, 基于网络的存储)，logging和metrics。</p>
</li>
</ul>
<p>如果背压发生，它会向上游传递最终会到sources，并使得sources减速。这对于资源不足的情况而言不是坏事。但是，如果想改进你的任务，从而使得在不消耗更多资源的情况下获得更高的负载。这样，就必须找出瓶颈在哪个地方和如何引起的。Flink提供两种机制来发现瓶颈：</p>
<ul>
<li><p>直接通过Flink的Web UI来监控背压</p>
</li>
<li><p>通过对网络的监控</p>
</li>
</ul>
<p>Flink的web UI可以最直接了解背压情况，但是它会有一些缺陷。网络监控对于持续监控的支持是非常好的，并且可以解释背压产生的原因。</p>
<h2 id="背压监控器"><a href="#背压监控器" class="headerlink" title="背压监控器"></a>背压监控器</h2><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/monitoring/back_pressure.html" target="_blank" rel="noopener">backpressure monitor</a>仅在Flink的Web UI中显示。它是被请求触发的。它通过Thread.getStackTrace()方法来采样task线程，并计算task被缓存请求block的采样结果。这些task要么不能以它们产生缓存的速度发送网络缓存(network buffer)，要么下游tasks基于信誉减速上游的发送速率。背压监控器将显示所有请求的block率。一些背压被认为是正常的/暂时的，总共有以下三种状态：</p>
<ul>
<li>OK。ratio &lt;= 0.10</li>
<li>LOW 0.10 &lt; ratio &lt;= 0.5</li>
<li>HIGH 0.5 &lt; ratio &lt;= 1</li>
</ul>
<p>虽然可以调整刷新间隔，采样数量和采样间的延迟参数，但是在正常情况下，使用默认值就可以。</p>
<p>背压监控器虽然可以找出背压的出处，但是不能找出原因。另外，对于大任务或者高并行的任务，背压监控器采集数据可能会比较慢。同时它也会影响任务的性能。</p>
<h2 id="网络监控-1"><a href="#网络监控-1" class="headerlink" title="网络监控"></a>网络监控</h2><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/monitoring/metrics.html#network" target="_blank" rel="noopener">Network</a>和<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/monitoring/metrics.html#io" target="_blank" rel="noopener">task I/O</a>监控是一种更加轻量级的监控，并且每个任务会持续输出这些指标。主要的指标如下：</p>
<ul>
<li><p>Flink1.9以上：outPoolUsage<code>,</code>inPoolUsage<code>,</code>floatingBuffersUsage<code>,</code>exclusiveBuffersUsage。</p>
<p>inPoolUsage是floatingBUffersUsage和exclusiveBuffersUsagee的总和。</p>
</li>
<li><p>numRecordsOut, numRecordsIn</p>
<p>每个指标有两个维度：算子维度和subtask维度。对于网络监控，使用subtask维度来展示它收到和发送的数据。同时 ，还需要关注一下对等的…PerSecond指标。</p>
</li>
<li><p>numBytesOut, numBytesInLocal, numBytesInRemote</p>
<p>subtask发出或者从本地或者远程读到的字节总和。同样有…PerSecond指标。</p>
</li>
<li><p>numBuffersOut, numBUffersInLocal, numBuffersInRemote</p>
<p>与numBytes…类似，但是计算的是网络缓存数。</p>
</li>
</ul>
<h3 id="背压"><a href="#背压" class="headerlink" title="背压"></a>背压</h3><p>背压可以通过两种不同的指标来展示：本地缓存池使用情况和输入/输出队列长度。它们的粒度不同。输入/输出队列长度指标因为内在的问题，这边暂不讨论。对于缓存池的情况：</p>
<ul>
<li><p>如果subtask的outPoolUsage是100%，它就被背压了。它是否已经block或者一直在等待数据进入网络缓存(network buffer)取决于这个缓存是有多满。</p>
</li>
<li><p>如果subtask的inPoolUsage是100%，意味着所有的floating buffer已经被分配到通道中去，而且产生的反压会向上游传递。这些floating buffers有以下几种情况：它们被exclusive buffer使用(远程输入通道总是尝试保持 exclusive buffer信誉)或者被backlog使用（包含数据和被装在输入通道中或者包含数据和正在被接收端的subtaask读取）。</p>
</li>
<li><p>Flink1.9以上：如果inPoolUsage总是在100%左右，就表明反压正在向上游传递。<br><img src="table4.png" alt=""><br>从上面还可以得出：</p>
</li>
<li><p>如果接收端所有的subtasks的inPoolUsage是低的，而且它上游的outPoolUsage是高的，那么背压可能是由网络瓶颈引起的。因为网络是共享资源，所以可能是自身占用的网络资源太多，或者是其他操作占用太多的网络资源。</p>
</li>
</ul>
<p>背压也有可能由一个task实例引发。这个task的所有分区可能正在执行耗时的操作或者是数据倾斜。</p>
<p>如果floatingBufferUsage不是100%，不太可能出现背压。如果floatingBufferUsage是100%，并且上游task也是背压的，往往可能是单个或多个输入通道在背压。可以通过exclusiveBuffersUsage来区分不同的情况：</p>
<ul>
<li>假设floatingBufferUsage是100%，exclusiveBuffersUsage越高，输入通道越有可能出现背压。在exclusiveBuffersUsage接近100%的极端情况下，可能所有的输入通道都是背压的。<br><img src="table5.png" alt=""><h3 id="资源使用-吞吐量"><a href="#资源使用-吞吐量" class="headerlink" title="资源使用/吞吐量"></a>资源使用/吞吐量</h3></li>
</ul>
<p>除了上面的指标使用，还可以与其他指标结合来查看网络的情况：</p>
<ul>
<li>低吞吐，经常性的outPoolUsage在100%附近，但是接收端inPoolUsage非常低：表明信誉通知的往返时间比较长。可以考虑增大<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/config.html#taskmanager-network-memory-buffers-per-channel" target="_blank" rel="noopener">buffers-per-channel</a> 参数或者尝试关闭流控。</li>
<li>与numRecordsOut和numBytesOut结合，可以算出序列化的数据的平均大小，来估算峰值场景。</li>
<li>如果想找出缓存填充率的原因和输出刷新的影响，可以结合numBytesInRemote和numBUffersInRemote。当调节吞吐量指标时，低的缓存填充率表明网络效率的降低。在这种场景下，考虑增加buffer超时时间。在Flink1.8或者1.9中，numBuffersOut仅在缓存满时或者遇到checkpoint barrier时才会增加，这会导致一定的延迟。本地通道的缓存填充率是不必要的，因为远程通道的优化技术对本地通道没什么用。</li>
<li>通常情况下，numBytesInLocal和numBytesInRemote分开计算没什么必要。</li>
</ul>
<h3 id="背压如何解决"><a href="#背压如何解决" class="headerlink" title="背压如何解决"></a>背压如何解决</h3><p>如果背压产生了，下一步就是分析它是如何产生的。首先需要检查基础的指标。</p>
<h4 id="系统资源"><a href="#系统资源" class="headerlink" title="系统资源"></a>系统资源</h4><p>首先，需要检查机器的资源利用率，比如CPU，网络或者磁盘I/O。如果资源已充分利用，可以尝试：</p>
<ul>
<li>优化代码</li>
<li>调优Flink的资源</li>
<li>扩大并行度或者增加机器的数量</li>
</ul>
<h4 id="垃圾回收"><a href="#垃圾回收" class="headerlink" title="垃圾回收"></a>垃圾回收</h4><p>有时性能问题是由GC引起的。可以通过-<code>XX:+PrintGCDetails</code>来打印GC日志，或者使用GC分析器。</p>
<h4 id="CPU-线程瓶颈"><a href="#CPU-线程瓶颈" class="headerlink" title="CPU/线程瓶颈"></a>CPU/线程瓶颈</h4><p>有时CPU瓶颈不太容易察觉。比如有些线程引起了瓶颈，但是整个机器的CPU利用率还是比较低的。可以考虑使用代码分析器来定位热线程。</p>
<h4 id="线程竞争"><a href="#线程竞争" class="headerlink" title="线程竞争"></a>线程竞争</h4><p>与CPU/线程瓶颈类似，subtask可能由于线程竞争导致瓶颈。CPU分析器可以帮助定位同步/锁竞争。</p>
<h4 id="负载不均衡"><a href="#负载不均衡" class="headerlink" title="负载不均衡"></a>负载不均衡</h4><p>如果由于数据倾斜引起，可以尝试改变数据分区(分开热点key或者是用local/pre-aggregation)。</p>
<h3 id="延迟追踪"><a href="#延迟追踪" class="headerlink" title="延迟追踪"></a>延迟追踪</h3><p>在低吞吐情况下，延迟通常被输出刷新器影响，可以通过调节缓存超时时间或者修改代码来改善。当处理一条数据的时间超过了期望值，或者多个计时器在同一时间触发，就会block接收端处理数据的能力。</p>
<p>Flink提供了 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/monitoring/metrics.html#latency-tracking" target="_blank" rel="noopener">tracking the latency</a>来追踪通过系统的数据。但是开关默认是关闭的。可以通过 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/config.html#metrics-latency-interval" target="_blank" rel="noopener"><code>metrics.latency.interval</code></a>或者ExecutionConfig#setLatencyTrackingInterval()来设置。开启后，Flink将会根据<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/config.html#metrics-latency-granularity" target="_blank" rel="noopener"><code>metrics.latency.granularity</code></a>来收集不同粒度的延迟矩形图。</p>
<ul>
<li>single：每个算子的subtask一个矩形图</li>
<li>operator(默认):每个source task和算子 subtask的并集一个矩形图</li>
<li>subtask：每个source subtask和算子subtask的并集一个矩形图</li>
</ul>
<p>这些指标通过特殊的”延迟标记”收集：每个source subtask将周期性发出一条延迟标记，它随着正常数据流动，但是不参与计算和缓存队列。延迟标记因此仅仅度量用户代码之间的等待时间，而无法做到端到端延迟追踪。而且，用户代码会间接的影响等待时间。<br>因为延迟标记像正常数据一样坐落在网络缓存中，它们可能会等待缓存被填满或者缓存超时。当一个通道是高负载的时候，不会增加网络缓存数据的延迟。但是，一旦通道是低负载的，数据和延迟标记将会有一个平均的buffer_timeout/2的延迟。这个延迟会被增加到每个网络连接当中去，因此在分析subtask的延迟指标时，需要把这个考虑进去。<br>通过查看每个subtask暴露的延迟追踪指标，应该可以定位到哪个subtask增加了整个链路的延迟。<br>开启延迟监控会显著影响集群性能，特别是subtask尺度。推荐在debug时使用。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文介绍了批处理和流处理不同的网络连接和调度类型。介绍了基于信誉的流控机制和内部网络栈工作的原理，以及它们的背压情况。通过背压监控器可以找到背压的源头，通过网络监控，结合其他系统指标，比如CPU、GC、线程等，可以找到背压的原因。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://flink.apache.org/2019/06/05/flink-network-stack.html" target="_blank" rel="noopener">A Deep-Dive into Flink’s Network Stack</a></p>
<p><a href="https://flink.apache.org/2019/07/23/flink-network-stack-2.html" target="_blank" rel="noopener">Flink Network Stack Vol. 2: Monitoring, Metrics, and that Backpressure Thing</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/02/20/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Flink%E7%BD%91%E7%BB%9C%E6%A0%88%E5%8F%8A%E8%83%8C%E5%8E%8B%E7%9B%91%E6%8E%A7/" data-id="ck736a2r90032ees6h9owhrhq" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Flink/" rel="tag">Flink</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-如何计算Flink集群规模：信封背计算法" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/11/08/%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97Flink%E9%9B%86%E7%BE%A4%E8%A7%84%E6%A8%A1%EF%BC%9A%E4%BF%A1%E5%B0%81%E8%83%8C%E8%AE%A1%E7%AE%97%E6%B3%95/" class="article-date">
  <time datetime="2019-11-08T01:31:32.000Z" itemprop="datePublished">2019-11-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/11/08/%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97Flink%E9%9B%86%E7%BE%A4%E8%A7%84%E6%A8%A1%EF%BC%9A%E4%BF%A1%E5%B0%81%E8%83%8C%E8%AE%A1%E7%AE%97%E6%B3%95/">如何计算Flink集群规模：信封背计算法</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>2017年柏林Flink Forward大会上Robert Metger的”Keep It Going: How to Reiably and Efficiently Operate Apache Flink”的演讲很受欢迎。Robert的其中一个主题演讲涉及到了如何估算Flink集群规模。Flink Forward大会的观众们认为这个计算方法对他们很有用，因此我们把他的演讲主题转变成这篇博客。</p>
<p>Flink社区上经常被问起的一个问题是当从开发转到线上时，如何估算集群规模大小。当然，最准确的的答案是根据需要，但是这并没有什么用。这篇博客提出了一系列问题使你能够计算出一些基准。</p>
<h2 id="1-通过数学建立基准"><a href="#1-通过数学建立基准" class="headerlink" title="1 通过数学建立基准"></a>1 通过数学建立基准</h2><p>首先，思考一下你的应用操作需要的资源基准的指标。</p>
<p>关键指标如下：</p>
<ul>
<li>每秒的数据量和每条数据的大小</li>
<li>去重key的数量和每个key的state大小</li>
<li>state更新的数量和state backend的方式</li>
</ul>
<p>最后，考虑一下你的服务等级协议(SLAS)，比如宕机时间、延迟和最大的吞吐量。这些指标将直接影响你的容量计算。</p>
<p>接下来，看一下基于预算可用的资源大小。比如：</p>
<ul>
<li>网络容量，需要考虑外部服务的网络消耗，比如Kakfa、HDFS等。</li>
<li>磁盘带宽，比如你使用磁盘的state backend，比如RocksDB。同时需要考虑外部服务的磁盘使用，比如Kafka、HDFS等。</li>
<li>机器的数量和它们的CPU和内存。</li>
</ul>
<p>基于上述这些因素，你现在能够估算正常流程的的资源基准。另外，还需要增加一些资源用作异常的恢复和checkpointing。</p>
<h2 id="2-样例：计算"><a href="#2-样例：计算" class="headerlink" title="2 样例：计算"></a>2 样例：计算</h2><p>我现在通过一个集群上的虚拟job部署来描述整个资源基准的建立过程。信封背计算法的所用到的数字是不精准的，同时并没有考虑的很全面。在后面，我会指出在做计算时的忽视的一些点。</p>
<h3 id="2-1-Flink流式应用样例和硬件"><a href="#2-1-Flink流式应用样例和硬件" class="headerlink" title="2.1 Flink流式应用样例和硬件"></a>2.1 Flink流式应用样例和硬件</h3><p><img src="How-to6-1.png" alt="Example Flink Streaming job topology"></p>
<p>在这个案列中，我将部署典型的Flink流式应用，Kafka Topic的数据作为数据源。这个流接着使用keyed， aggregating window操作转换。窗口操作执行5分钟的窗口聚合。同时假设有源源不断的数据进来，window被设置成1分钟滑动一次。</p>
<p>这表示每分钟执行一次过去5分钟内的窗口聚合。这个流式应用根据userId字段进行聚合。Kafka Topic中消息的大小平均是2KB。</p>
<p>吞吐量是每秒100万条消息。为了理解窗口操作的state大小，你需要知道distinct Keys的数量，就是userIds的数量，这边大约是5000万个不同的用户ID。对于每个用户，你需要计算4个数字，通过longs(8 byte)存储。</p>
<p>现在，让我们总结一下这个任务的关键指标：</p>
<ul>
<li>消息大小：2KB</li>
<li>吞吐量：1000000 msg/sec</li>
<li>Distinct Keys: 500000000(窗口聚合：每个key4个long大小)</li>
<li>Checkpointing: 每分钟一次</li>
</ul>
<p>— 硬件：</p>
<ul>
<li>5台机器</li>
<li>10 gigabit 以太网</li>
<li>每台机器运行一个Flink TaskManager</li>
<li>磁盘通过网络挂载</li>
</ul>
<p>— Kafka 独立部署</p>
<p><img src="How-to0-1.png" alt="假设硬件步骤"></p>
<p>总共有5台机器运行这个job，每台机器上运行一个TaskManager。磁盘通过网络挂载，同时有10 gigabit的以太网接入。同时Kafka是独立部署在其他机器上。</p>
<p>每台机器有16CPU核。为了简化的需要，这边不考虑CPU和内存的使用情况。在实际情况下，你需要根据应用逻辑和state backend的使用，来考虑内存的使用。这个例子使用RocksDB state backend。（它是健壮的，同时对内存需求比较低）。</p>
<h3 id="2-2-单机计算"><a href="#2-2-单机计算" class="headerlink" title="2.2 单机计算"></a>2.2 单机计算</h3><p>为了理解整个job运行部署的资源需求，最容易的方式是关注单台机器和TaskManager的操作。你可以通过单台机器计算出来的数字来推断整个集群的资源需求。</p>
<p>默认(所有的操作都有并行度和没有特殊的调度限制)所有的操作在每台机器上都有运行。</p>
<p>在这个例子中，Kafka source， 窗口操作和Kafka sink都运行在每台机器上。</p>
<p><img src="How-to9-1.jpg" alt="A machine perspective - TaskManager n"></p>
<p>keyBy是一个分离的操作，因此资源需求计算比较容易。在现实中，keyBy是一个API，连接了Kafak Source和窗口操作。</p>
<p>我现在将从头到底理解这些操作的网络资源需求。</p>
<h3 id="2-3-Kafka-source"><a href="#2-3-Kafka-source" class="headerlink" title="2.3 Kafka source"></a>2.3 Kafka source</h3><p>为了计算Kafka source收到的数据量，首先需要计算Kafka的聚合输入。sources每秒收到100万消息，每条消息2KB大小。<br>$$<br>2KB \times 1000000/s = 2GB/s<br>$$<br>2GB/s除以5台机器，得到如下结果：<br>$$<br>2GB/s \div 5 machines = 400MB/s<br>$$<br>集群中每台机器上TaskManager的source收到400MB/s的数据。</p>
<p><img src="How-to10-1.png" alt="Kafka source calculation"></p>
<h3 id="2-4-Shuffle-keyBY"><a href="#2-4-Shuffle-keyBY" class="headerlink" title="2.4 Shuffle / keyBY"></a>2.4 Shuffle / keyBY</h3><p>接下来，你需要确保同一个key的所有事件落在一些机器上。这边，你从kafka中读取的数据可能被重新分区。<br>shuffle过程发送所有拥有相同key的数据到同一台机器，因此这边把400MB/s的数据分割成一个根据userId分区的流。<br>$$<br>400MB/s \div 5 machines = 80MB/s<br>$$<br>平均来看，你将发送80MB/s的数据到每一台机器。这个分析是从单台机器的角度，但是一些数据已经在目标机器上了，因此要减去80MB/s。<br>$$<br>400MB/s - 80MB/s = 320MB/s<br>$$<br><img src="How-to8-1.png" alt="shuffle calculation"></p>
<h3 id="2-5-Window-Emit-and-Kafka-Sink"><a href="#2-5-Window-Emit-and-Kafka-Sink" class="headerlink" title="2.5 Window Emit and Kafka Sink"></a>2.5 Window Emit and Kafka Sink</h3><p>接下去的问题是窗口操作发送多少数据到Kafka Sink。结果是67MB/s，让我们看一下如何计算。<br>窗口操作为每个key保持了4个数字(longs)聚合。每一分钟，操作将发送当前的聚合值。每个key发送2ints(user_id, window_ts)和4 longs。<br>$$<br>(2 \times4bytes) + (4\times8bytes) = 40 bytes , per, key<br>$$<br>然后乘以keys数量(500000000除以机器数量)<br>$$<br>500000000 \div5machines \times40bytes = 40GB<br>$$<br>然后计算每秒的大小：<br>$$<br>40GB/min \div60=67MB/s<br>$$<br>这表示每个TaskManager从窗口操作中平均发送67MB/s的用户数据。因为Kafka sink运行在每个TaskManager上，所以没有进一步的分区操作。这就是从Flink到Kafka的发送的数据量。</p>
<p><img src="How-to5-1.png" alt="User data: From Kafka, shuffled to the window operators and back to Kafka"></p>
<p>从窗口操作中得到的数据每分钟会发送一次。在实际中，这个操作不会发以67MB/s的发送数据，而是在一分钟之内的几秒间到达最大带宽。</p>
<p>现在，总结一下：</p>
<ul>
<li>进来的数据：720MB/s(400+320)per machine</li>
<li>出去的数据：387MB/s(320+67)per machine</li>
</ul>
<p><img src="How-to2-1.png" alt=""></p>
<h3 id="2-6-State和Checkpointing"><a href="#2-6-State和Checkpointing" class="headerlink" title="2.6 State和Checkpointing"></a>2.6 State和Checkpointing</h3><p>到目前为止，我们仅仅计算了Flink处理的用户数据。你同时还需要考虑磁盘的使用，比如存储state 和checkpointing。为了计算磁盘的花销，你需要查看窗口计算如何进入state。Kafka Source也需要保持一些state，但是跟窗口操作的state相比，可以忽略不计。</p>
<p>为了理解窗口操作的state大小，让我们换一个角度看这个问题。Flink计算5分钟的时间窗口，并且1分钟滑动一次。Flink是通过保持5个窗口来实现滑动窗口。根据先前提到的，在使用窗口时，你需要为每个窗口保持40bytes的状态，并且窗口是提前聚合的。对于每一条到来的事件，你首先需要取出当前聚合值，再更新聚合值，然后把新值写回去。</p>
<p><img src="How-to7-1.png" alt="Window State"></p>
<p>这意味着：<br>$$<br>40 , bytes ,of ,state \times 200000 msg/s , per , machine = 40MB/s<br>$$</p>
<p>有40MB/s的磁盘读写（每台机器上）。根据先前说的，磁盘是通过网络挂载的。因此需要在先前的基础上增加这个值。</p>
<p>现在总共需要的资源如下：</p>
<ul>
<li>进入的数据：760MB/s(400MB/s data in + 320MB/s  shuffle + 40MB/s state)</li>
<li>出去的数据：427MB/s(320MB/s shuffle + 67MB/s data out + 40MB/s state)<br><img src="How-to3-1.png" alt=""></li>
</ul>
<p>上述的计算考虑了state的进入，当事件到达窗口操作时触发。你还需要checkpoint和容错机制。如果一台机器或者其他任何东西挂掉，你想要恢复你的窗口并继续处理。</p>
<p>Checkpointing是每隔1分钟执行一次，并且每个checkpoint复制整个job的状态到通过网络挂载的文件系统。</p>
<p>让我们快速的看一下每台机器的state大小：<br>$$<br>40 , bytes , of , state \times 5 , windows \times100000000 , keys = 20GB<br>$$<br>接着算每秒的值：<br>$$<br>20GB \div 60 =333MB/s<br>$$<br>和窗口操作类似，checkpointing也是每分钟执行一次。它尝试全速发送数据到外部存储。Checkpointing引起了额外的state进入。自从Flink1.3后，RocksDB支持增量checkpointing来降低每次checkpoint时所需的网络传输。</p>
<p>计算更新如下：</p>
<ul>
<li>进入的数据：760MB/s(400 + 320 + 40)</li>
<li>出去的数据：760MB/s(320 + 67 + 40 + 333)</li>
</ul>
<p><img src="How-to2-1.png" alt="Window State"></p>
<p>这意味着整个集群网络流量是：<br>$$<br>(760 + 760)\times5 + 400 + 2335=10335MB/s<br>$$<br>400是80MB的state读写乘以5台机器。2335是Kafka进和出的总值。</p>
<p>整个硬件的网络可用容量如下：</p>
<p><img src="How-to4-1.png" alt="Networking requirements"></p>
<p>这边我要加一个免责声明。上述这些计算没有包含协议的花销，比如TCP、Ethernet和RPC（在Flink、Kafka和HDFS等中）。但是上述的计算仍旧对如何计算一个job的资源有指导意义。</p>
<h2 id="Scale-Your-Way"><a href="#Scale-Your-Way" class="headerlink" title="Scale Your Way"></a>Scale Your Way</h2><p>基于我的分析，这个例子中，5个节点的集群，在典型的操作中，每个机器需要处理760MB/s的数据进出，同时每台机器可以处理的容量是1250MB/s。这样保留了40%的网络容量来应对我刚才提到的复杂度，比如网络协议花销，事件重放，数据倾斜引起的不平均的负载等。</p>
<p>当然，没有一个标准答案来说明留40%的余量是否合适。但是这个算法可以给你一个好的开始。尝试上述的计算，修改上述的参数为你自己的参数。Happy scaling！</p>
<h2 id="翻译源"><a href="#翻译源" class="headerlink" title="翻译源"></a>翻译源</h2><p><a href="https://www.ververica.com/blog/how-to-size-your-apache-flink-cluster-general-guidelines" target="_blank" rel="noopener">How To Size Your Apache Flink® Cluster: A Back-of-the-Envelope Calculation</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/11/08/%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97Flink%E9%9B%86%E7%BE%A4%E8%A7%84%E6%A8%A1%EF%BC%9A%E4%BF%A1%E5%B0%81%E8%83%8C%E8%AE%A1%E7%AE%97%E6%B3%95/" data-id="ck736a2r2002gees6089xdmrs" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Flink/" rel="tag">Flink</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-如何搭建高可用Hadoop集群" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/22/%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8Hadoop%E9%9B%86%E7%BE%A4/" class="article-date">
  <time datetime="2018-12-22T11:21:10.000Z" itemprop="datePublished">2018-12-22</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/22/%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8Hadoop%E9%9B%86%E7%BE%A4/">如何搭建高可用Hadoop集群</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>在这篇文章中，我将讲述HDFS 2.x高可用集群架构和如何搭建一个高可用的HDFS集群。其内容如下：</p>
<ul>
<li><p>HDFS HA架构</p>
<ul>
<li>介绍</li>
<li>NameNode高可用</li>
<li>HA架构</li>
<li>HA的实现（JournalNode和共享存储）</li>
</ul>
</li>
<li><p>如何设置Hadoop集群的HA(Quorum Journal Nodes)？</p>
</li>
</ul>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>为了解决Hadoop1.x的单点故障问题，Hadoop2.x引入了高可用集群的概念。HDFS架构遵从Master/Slave拓扑，NameNode作为Master负责管理Slave节点(DataNode)。由于Master只有一个节点，因此它将成为整个系统的瓶颈。尽管引入的Secondary NameNode降低了NameNode中数据丢失的风险，但是它并没有解决NameNode高可用的问题。</p>
<h2 id="NameNode可用性"><a href="#NameNode可用性" class="headerlink" title="NameNode可用性"></a>NameNode可用性</h2><p>如果使用HDFS的默认配置，NameNode会有单点故障的风险。当NameNode不可用的时候，整个系统将会是不可用的，直到NameNode重启或者引入新的NameNode。</p>
<p>NameNode的不可用的原因可能是以下几个方面：</p>
<ul>
<li>软件或者硬件的升级</li>
<li>NameNode的崩溃</li>
</ul>
<h2 id="HDFS高可用架构"><a href="#HDFS高可用架构" class="headerlink" title="HDFS高可用架构"></a>HDFS高可用架构</h2><p>HA架构通过设置两个NameNode(一主一备)来解决NameNode的可用性问题。因此，在一个高可用的集群中，将同时会有两个NameNode在运行。</p>
<ul>
<li>Active NameNode</li>
<li>Standby/Passive NameNode</li>
</ul>
<p><img src="HDFS-HA-Architecture.png" alt="HDFS-HA-Architecture"></p>
<p>如果一个NameNode挂了，另一个NameNode将会接管整个集群，从而减少集群的下线时间。备用的NameNode拥有集群的失败保护能力。因此当主节点挂了之后，通过被节点我们能够自动失败恢复。</p>
<p>但是，在保持HDFS高可用集群的一致性上，将会有以下两个问题：</p>
<ul>
<li>主备节点相互同步。拥有相同的元数据，可以使得主节点挂了之后，备节点可以快速的失败恢复。</li>
<li>同时只能有一个主节点。当有两个主节点时，将会导致数据竞争。一个集群被分裂成两个小集群，这种场景被称为脑裂。为了避免这种场景的发生，必须建立一个栅栏来确保同时只有一个主节点。</li>
</ul>
<h2 id="HA架构的实现"><a href="#HA架构的实现" class="headerlink" title="HA架构的实现"></a>HA架构的实现</h2><p>在HDFS高可用架构中，同时运行有两个NameNode，一个主，一个备。它通过下面两种方式来实现主备节点配置。</p>
<ul>
<li>使用Quorum Journal Nodes</li>
<li>使用NFS共享存储</li>
</ul>
<h3 id="Quorum-Journal-Nodes"><a href="#Quorum-Journal-Nodes" class="headerlink" title="Quorum Journal Nodes"></a>Quorum Journal Nodes</h3><p><img src="JournalNode-HDFS-HA-Architecture.png" alt="JournalNode-HDFS-HA-Architecture"></p>
<ul>
<li>主备节点通过一组独立的叫做JournalNodes的节点来保持数据同步。JournalNodes之间相互连接，然后把收到的请求信息复制到其他节点。</li>
<li>主NameNode负责更新JournalNodes中的EditLogs（元信息）。</li>
<li>备NameNode读取JournalNode中EditLogs中的变化，并且应用到它自己的命名空间。</li>
<li>在失败恢复期间，备NameNode需要确保在称为主节点之间更新JournalNodes中的元信息。这使得当前的命名空间与失败之前的命名空间状态同步。</li>
<li>两个NameNode的IP地址对于所有的DataNodes都是可见的。同时DataNodes发送心跳包和阻塞位置信息给两个NameNode。这样，由于备节点及时更新了集群的额阻塞信息，从而可以更快的恢复。</li>
</ul>
<h4 id="NameNode隔离"><a href="#NameNode隔离" class="headerlink" title="NameNode隔离"></a>NameNode隔离</h4><p>之前讨论过，同时只有一个主NameNode对整个集群来说是非常重要的。因此，NameNode之间的隔离就显得尤为重要。</p>
<ul>
<li>JournalNodes只允许同时只有一个NameNode可以写日志。</li>
<li>主节点挂之后，备节点负责写日志到JournalNodes，并且禁止其他NameNode再成为主节点。</li>
<li>最后，备节点就转换为主节点。</li>
</ul>
<h3 id="使用共享存储"><a href="#使用共享存储" class="headerlink" title="使用共享存储"></a>使用共享存储</h3><p><img src="Shared-Storage-HDFS-HA-Architecture.png" alt="Shared-Storage-HDFS-HA-Architecture"></p>
<ul>
<li>主备NameNode节点通过共享存储来保持同步。主NameNode把它在自己的命名空间中修改的日志打到共享存储的EditLog中。备节点读取共享存储中的EditLog，并更新到自己的命名空间中。</li>
<li>在失败恢复的时候，备节点首先使用共享存储中的EditLog来更新元信息。然后，它就成为了主节点。这样，就使得当前的命名空间的状态跟失败恢复之前的状态同步。</li>
<li>管理员必须配置至少一个栅栏方法来防止脑裂场景。</li>
<li>系统可以使用很多栅栏隔离机制。比如杀死NameNode的进程和废除进入共享存储目录的权限。</li>
<li>另外，我们可以使用STONITH技术来隔离先前的主节点。STONOTH使用专门的分布式单元来强制下线主NameNode机器。</li>
</ul>
<h3 id="自动失败恢复"><a href="#自动失败恢复" class="headerlink" title="自动失败恢复"></a>自动失败恢复</h3><p>失败恢复是指一种当检测到失败时，自动转换到次要系统。总共有两种失败恢复：</p>
<ul>
<li>优雅的失败恢复：手动初始化失败恢复。</li>
<li>自动失败恢复：当NameNode挂了之后，自动初始化失败恢复程序。</li>
</ul>
<p>Apache Zookeeper为HDFS高可用集群提供自动失败恢复的能力。它保持非常小的协调数据，通知客户端数据改变和监测客户端的失败。Zookeeper保持与NameNode的会话。在失败的情况下，会话将会过期，Zookeeper将通知其他NameNode初始化失败恢复程序。同时，其他备NameNode将会锁住Zookeeper，来使得自己成为主NameNode。</p>
<p>ZookeerFailoverController (ZKFC) 是一个Zookeeper客户端，能够监测和管理NameNode的状态。每个NameNode运行一个ZKFC。ZKFC负责周期性的监测NameNodes的健康状态。</p>
<p>现在我们了解了高可用的Hadoop集群，接下来就开始部署集群。</p>
<p>在主NameNode运行的后台程序有：</p>
<ul>
<li>Zookeeper Fail Over controller</li>
<li>JournalNode</li>
<li>NameNode</li>
</ul>
<p>备NamdeNode运行的后台程序有：</p>
<ul>
<li>Zookeeper Fail Over controller</li>
<li>JournalNode</li>
<li>NameNode</li>
</ul>
<p>DataNode上运行的后台程序有：</p>
<ul>
<li>JournalNode</li>
<li>DataNode</li>
</ul>
<h3 id="搭建和配置Hadoop的高可用集群"><a href="#搭建和配置Hadoop的高可用集群" class="headerlink" title="搭建和配置Hadoop的高可用集群"></a>搭建和配置Hadoop的高可用集群</h3><p>1.每个节点设置JAVA环境和host。</p>
<table>
<thead>
<tr>
<th>虚拟机</th>
<th>IP地址</th>
<th>主机名</th>
</tr>
</thead>
<tbody><tr>
<td>主NameNode</td>
<td>192.168.60.1</td>
<td>hadoop01</td>
</tr>
<tr>
<td>备NameNode</td>
<td>192.168.60.2</td>
<td>hadoop02</td>
</tr>
<tr>
<td>DataNode</td>
<td>192.168.60.3</td>
<td>hadoop03</td>
</tr>
</tbody></table>
<p>2.每个节点设置ssh免密登录。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ssh-keygen -t rsa</span></span><br></pre></td></tr></table></figure>

<p>一直按enter直到结束。当都生成ssh密钥后，你会得到一个公钥和私钥。</p>
<p><strong>注意：.ssh目录的权限需要设置为700，.ssh目录里面文件的权设置为600。</strong></p>
<p>然后把每台机器上的id_rsa.pub内容复制到authrozied_keys文件中。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ssh-copy-id –i .ssh/id_rsa.pub root@hadoop02</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ssh-copy-id –i .ssh/id_rsa.pub root@hadoop03</span></span><br></pre></td></tr></table></figure>

<p>接着重启所有节点的ssh服务。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> service sshd restart</span></span><br></pre></td></tr></table></figure>

<p>接着你可以通过ssh命令免密登录到其他机器上。如果还是需要密码，请检查以上设置是否正确。</p>
<p>3.下载和解压Hadoop和Zookeeper的二进制文件。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> wget https://archive.apache.org/dist/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> wget https://archive.apache.org/dist/hadoop/core/hadoop-2.6.0/hadoop-2.6.0.tar.gz</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> tar –xvf zookeeper-3.4.6.tar.gz</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> tar –xvf hadoop-2.6.0.tar.gz</span></span><br></pre></td></tr></table></figure>

<p>4设置hadoop环境。打开.bashrc文件。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vi ~/.bashrc</span></span><br></pre></td></tr></table></figure>

<p>添加以下内容：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=&lt; Path to your Hadoop-2.6.0 directory&gt;</span><br><span class="line">export HADOOP_MAPRED_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_COMMON_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_HDFS_HOME=$HADOOP_HOME</span><br><span class="line">export YARN_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br><span class="line">export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br><span class="line">export JAVA_HOME=&lt;Path to your Java Directory&gt;</span><br><span class="line">export ZOOKEEPER_HOME =&lt;Path to your Zookeeper Directory&gt;</span><br><span class="line">export PATH=$PATH: $JAVA_HOME/bin: $HADOOP_HOME/bin: $HADOOP_HOME/sbin:$ZOOKEEPER_HOME/bin</span><br></pre></td></tr></table></figure>

<p>5.设置hadoop配置文件</p>
<p>编辑core-site.xml文件，增加以下内容。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>Athena<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://ha-cluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	</span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/HA/jn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>编辑hdfs-site.xml文件，增加以下内容</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/HA/namenode<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span>ha-cluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.ha-cluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ha-cluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop01:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ha-cluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ha-cluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop01:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ha-cluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://hadoop01:8485;hadoop02:8485;hadoop03:8485/ha-cluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.ha-cluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span> hadoop01:2181,hadoop02:2181,hadoop03:2181 <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span>sshfence<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/root/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/HA/app/name-ha<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/HA/app/tmp-ha<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>6.设置zookeeper配置文件</p>
<p>编辑zoo.cfg，增加以下内容</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Server.1=hadoop01:2888:3888</span><br><span class="line">Server.2=hadoop02:2888:3888</span><br><span class="line">Server.3=hadoop03:2888:3888</span><br></pre></td></tr></table></figure>

<p>7.拷贝文件到各个节点</p>
<p>完成上述步骤后，把上面设置过的.bashrc文件，hadoop目录，zookeeper目录通过scp命令复制到其他节点。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> scp –r &lt;path of directory&gt; edureka@&lt;ip address&gt;:&lt;path <span class="built_in">where</span> you need to copy&gt;</span></span><br></pre></td></tr></table></figure>

<p>8.为zookeeper节点设置id。</p>
<p>在zookeeper的配置文件目录中，增加一个myid的文件。然后每个节点分别设置为1，2，3。</p>
<p>9.启动hadoop集群</p>
<p>在配置了journalnode的节点上启动journalnode。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop-daemon.sh start journalnode</span><br></pre></td></tr></table></figure>

<p>在主NameNode节点上格式化HDFS，并启动NameNode。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ HDFS namenode -format</span><br><span class="line">$ hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure>

<p>复制HDFS元信息到备NameNode，并启动备NameNode。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ HDFS namenode -bootstrapStandby</span><br><span class="line">$ hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure>

<p>在每个节点上启动zookeeper。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ zkServer.sh start</span><br></pre></td></tr></table></figure>

<p>在启动zookeeper服务后，你在每个节点上通过JPS命令可以看到QuorumPeerMain进程。</p>
<p>启动DataNode。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop-daemon.sh start datanode</span><br></pre></td></tr></table></figure>

<p>格式化zookeeper数据。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ HDFS zkfc –formatZK</span><br></pre></td></tr></table></figure>

<p>在主备NameNode启动启动ZKFC服务。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop-daemon.sh start zkfc</span><br></pre></td></tr></table></figure>

<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.edureka.co/blog/how-to-set-up-hadoop-cluster-with-hdfs-high-availability/" target="_blank" rel="noopener">How to Set Up Hadoop Cluster with HDFS High Availability</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/22/%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8Hadoop%E9%9B%86%E7%BE%A4/" data-id="ck736a2r4002nees65yr8flrh" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Akka在Flink中的应用" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/13/Akka%E5%9C%A8Flink%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/" class="article-date">
  <time datetime="2018-11-13T07:11:08.000Z" itemprop="datePublished">2018-11-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/11/13/Akka%E5%9C%A8Flink%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/">Akka在Flink中的应用</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>这篇文章主要介绍了Flink通过Akka实现的分布式通信。它第一次在0.9版本中出现。通过Akka，所有的远程程序调用被封装为异步消息。它主要涉及到JobManager、 TaskManager和JobClient三个组件。将来，它很可能在更多的组件中使用，从而使得它们可以发布和处理异步消息。</p>
<h2 id="Akka和Actor模型"><a href="#Akka和Actor模型" class="headerlink" title="Akka和Actor模型"></a>Akka和Actor模型</h2><p>Akka是一个并行的、容错的和可扩容的的框架。它实现了Actor模型，与Erlang的并行模型类似。在Actor模型中，所有的实体被当作独立的actor。Actor之间通过发送异步消息来相互通信。Actor模型受异步机制启发。同时，它可以等待一个同步操作的响应。但是一般不建议使用同步消息，因为它们限制了系统的扩展性。每个Actor有一个信箱用来存储收到的消息。每个actor独自维持自己的状态。下面是actors的网络模型图：</p>
<p><img src="actorNetwork.png" alt="actorNetwork"></p>
<p>每个Actor有一个单独的线程来拉取信箱中的消息并处理。作为已处理消息的结果，Actor可以改变其内部的状态，发送新的消息或者产生新的actors。如果这个内部状态的改变是专门线程处理的，那就不需要线程安全。尽管一个单独的actor是序列的，但是一个由一系列actor组成的系统就是高并发和可扩容的，因为处理线程在所有actor之间是共享的。这种分享也就是为什么不应该在actor线程中阻塞调用的原因。这种阻塞调用会阻止线程被其他actor用来处理自己的消息。</p>
<h2 id="Actor系统"><a href="#Actor系统" class="headerlink" title="Actor系统"></a>Actor系统</h2><p>一个Actor系统包含了所有存活的actors。它提供的共享服务包括调度、配置和日志等。Actor系统同时包含一个线程池，所有actor从这里获取线程。</p>
<p>多个Actor系统可以在一台机器上共存。如果一个Actor系统通过RemoteActorRefProvider启动，它就可以被其他机器上的Actor系统发现。Actor系统能够自动识别消息是发送给本地机器还是远程机器的Actor系统。在本地通信的情况下，消息通过共享存储器高效的传输。在远程通信的情况下，消息通过网络栈发送。</p>
<p>所有Actors都是继承来组织的。每个新创建的actor将其创建的actor视作父actor。继承被用来监督。每个父actor对自己的子actor负责监督。如果在一个子actor发生错误，父actor将会收到通知。如果这个父actor可以解决这个问题，它就重新启动这个子actor。如果这个错误父actor无法处理，它可以把这个错误传递给自己的父actor。</p>
<p>第一个actor通过系统创建，由/user 这个actor负责监督。详细的Actor的继承制度可以参考[这边]( Escalating an error simply means that a hierarchy layer above the current one is now responsible for resolving the problem. Details about Akka’s supervision and monitoring can be found <a href="http://doc.akka.io/docs/akka/snapshot/general/supervision.html" target="_blank" rel="noopener">here</a>.)。</p>
<h2 id="Flink中的Actors"><a href="#Flink中的Actors" class="headerlink" title="Flink中的Actors"></a>Flink中的Actors</h2><p>Actor是一个包含状态和行为的容器。actor线程顺序处理收到的消息。这样就让用户摆脱锁和线程管理的管理，因为一次只有已给线程对一个actor有效。但是，必须确保只有这个actor线程可以处理其内部状态。Actor的行为由receive函数定义，该函数包含收到的消息的处理逻辑。</p>
<p>Flink系统由3个分布式组件构成：JobClient，JobManager和TaskManager。JobClient从用户处得到Flink Job，并提交给JobManager。JobManager策划这个job的执行。首先，它分配所需的资源，主要就是TaskManagers上要执行的slot。</p>
<p>在资源分配之后，JobManager部署单独的任务到响应的TaskManager上。一旦收到一个任务，TaskManager产生一给线程用来执行这个任务。状态的改变，比如开始计算或者完成计算，将被发送回JobManager。基于这些状态的更新，JobManager将引导这个job的执行直到完成。一旦一个job被执行完，其结果将会被发送回JobClient。Job的执行图如下所示：</p>
<p><img src="jobExecutionProcess.png" alt="jobExecutionProcess"></p>
<h3 id="JobManager和TaskManager"><a href="#JobManager和TaskManager" class="headerlink" title="JobManager和TaskManager"></a>JobManager和TaskManager</h3><p>JobManager是核心控制单元，负责执行整个Flink Job。它掌管资源分配，任务调度和状态汇报。</p>
<p>在一个Flink Job可以被执行之前，一个或则多个TaskManager需要被启动。TaskManager通过发送一个RegisterTaskManager消息给JobManager来注册。JobManager然后发送一个AcknowledgeRegistration消息来确认成功注册。以防TaskManager已经被JobManager注册，因为会有多个RegisterTaskManager消息被发送的情况，JobManager将会收到一个AlreadyRegistered消息。如果这个注册被拒绝，JobManager会收到一个RefuseRegistration消息。</p>
<p>通过发送一个SubmitJob消息和对应的JobGraph，一个job被提交给JobManager。在收到JobGraph后，JobManager创建一个ExecutionGraph，它是JobGraph的并行版本。这个ExecutionGraph包含部署到TaskManager的相关信息。</p>
<p>JobManager的调度器负责在可用的TaskManagers上分配执行的slots。在TaskManager上分配执行slot之后，SubmitTask消息和所有必要的信息被发送给相关的TaskManager。成功部署后会发送一个TaskOperationResult消息。一旦提交的任务被成功部署运行，那么这个job提交就被认为是成功的。JobManager通过发送一条success消息和对应的job id来通知JobClient。</p>
<p>TaskManagers上的运行任务的状态更新通过UpdateTaskExecutionstate消息来通知JobManager。通过这些更新消息，ExecutionGraph就能够更新反映当前的执行状态。</p>
<p>JobManager还充当数据源的输入拆分分配器。它负责在所有TaskManager中分配工作，以便尽可能保留数据的本地性。为了动态平衡负载，Tasks在完成处理一个数据后，会请求一个新的输入拆分。这个请求通过发送RequestNextInputSplit消息给JobManager来实现。JobManager将会回应一个NextInputSplit消息。如果没有更多的输入拆分，输入拆分包含的消息就是null。</p>
<p>Tasks在TaskManagers上是懒部署的。这意味着在生产者生产了一些数据之后，Tasks才会被部署。一旦生产者产生数据了，它会发送一个ScheduleOrUpdateConsumers消息给JobManager。这条消息意味着消费者现在可以读取新的数据了。如果消费task还没开始，它就会在TaskManger上部署。</p>
<h3 id="JobClient"><a href="#JobClient" class="headerlink" title="JobClient"></a>JobClient</h3><p>JobClient是用户面对的组件。它用来和JobManager通信，并负责提交Flink jobs，查询提交任务的状态和接收当前运行任务的状态信息。</p>
<p>JobClient同样也是一个Actor。这边存在2种关于任务提交的消息：SubmitJobDetached和SubmitJobWait。第一个消息提交任务和退出注册接收到的状态消息和任务结果。detached模式一般用于提交忘记模式。</p>
<p>SubmitJobWait消息提交任务给JobManager并且注册收到状态消息。内部的实现是通过产生一个helper actor来处理收到的状态消息。一旦这个任务被终止，JobManager发送JobResultSuccess消息、执行时间和累加结果给helper actor。一旦收到这个消息，这个helper actor传递这个消息给client。</p>
<h3 id="异步-vs-同步消息"><a href="#异步-vs-同步消息" class="headerlink" title="异步 vs 同步消息"></a>异步 vs 同步消息</h3><p>在任何地方，Flink尝试使用异步消息和通过futures来处理响应。Futures和很少的几个阻塞调用有一个超时时间，以防操作失败。这是为了防止死锁，当消息丢失或者分布式足觉crash。但是，如果在一个大集群或者慢网络的情况下，超时可能会使得情况更糟。因此，操作的超时时间可以通过“akka.timeout.timeout”来配置。</p>
<p>在两个actor可以通信之前，需要获取一个ActorRef。这个操作的查找同样需要一个超时。为了使得系统尽可能快速的失败，如果一个actor还没开始，超时时间需要被设置的比较小。为了以防经历查询超时，你可以通过“akka.lookup.timeout”配置增加查询时间。</p>
<p>Akka的另一个特点是限制发送的最大消息大小。原因是它保留了同样数据大小的序列化buffer和不想浪费空间。如果你曾经遇到过传输失败，因为消息超过了最大大小，你可以增加“akka.framesize”配置来增加大小。</p>
<h3 id="失败检测"><a href="#失败检测" class="headerlink" title="失败检测"></a>失败检测</h3><p>失败检测对分布式系统健壮性是非常重要的。当线上集群在运行的时候，总是会发生组件失败或者不可达。这种失败的原因是多方面的，可以从硬件问题到网络问题。健壮的分布式系统应该能够检测到失败的组件并恢复它。</p>
<p>Flink通过Akka的DeathWatch机制来检测失败的组件。DeathWatch允许actors查看其他的actors，即使这些actors不被这个actor监视，或者存活在另外一个actor系统中。一旦一个被监视的actor死亡或者不可达，一个结束的消息将被发送给这个watcher actor。当收到这条消息，系统可以进一步处理它。在内部，DeathWatch被当作一个心跳和一个失败检测器，基于这个心跳的间隔，心跳暂停和失败阀值，评估actor可能什么时候死亡的。心跳间隔可通过”akka.watch.heartbeat.pause”来配置。通过”akka.watch.heartbeat.pause”设置心跳暂停。心跳暂停应该要是心跳间隔的倍数，否则失败的心跳会触发DeathWatch。失败阀值可以通过”akka.watch.threshold”配置来设置。更多关于DeathWatch细节和失败检测能够在<a href="http://doc.akka.io/docs/akka/snapshot/scala/remoting.html#watching-remote-actors" target="_blank" rel="noopener">这边</a>找到。</p>
<p>在Flink，JobManager观察所有注册的TaskManager，TaskManager观察其所对应的JobManager。通过这样，两个组件可以知道对方是否可达。JobManager在观察到TaskManager挂掉后后续就不会把任务派发给它。此外，JobManager还把挂掉的TaskManager上任务失败处理，同时在其他TaskManager上重新调度执行。为了以防短暂的连接丢失导致的TaskManager标记为挂掉，TaskManager可以在连接被重新建立之后简单的在JobManager上尝试重新注册。</p>
<p>TaskManager同样观察JobManager。当它检测到JobManager挂掉后，TaskManager失败处理所有的task，并且清空状态。此外，TaskManager将尝试重新连接JobManager，以防网络抖动导致的JobManager挂掉。</p>
<h3 id="未来的开发"><a href="#未来的开发" class="headerlink" title="未来的开发"></a>未来的开发</h3><p>当前，只有3个组件，JobClient，JobManager和TaskManager使用Actors实现通信。为了更好的探索并行，需要更多的组件作为actors参与其中。未来，ExecutionGraph的ExecutionVertices，或者执行对象的通信也可以作为actors。这种细粒度的actor模型的优点是状态更新可以直接发送到相应的Execution对象。通过这种方式，JobManager可以从单点通信中解脱出来。</p>
<h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><ul>
<li><code>akka.ask.timeout:</code>Timeout used for all futures and blocking Akka calls. If Flink fails due to timeouts then you should try to increase this value. Timeouts can be caused by slow machines or a congested network. The timeout value requires a time-unit specifier (ms/s/min/h/d) (DEFAULT: <strong>100 s</strong>).</li>
<li><code>akka.lookup.timeout:</code>Timeout used for the lookup of the JobManager. The timeout value has to contain a time-unit specifier (ms/s/min/h/d) (DEFAULT: <strong>10 s</strong>).</li>
<li><code>akka.framesize</code>: Maximum size of messages which are sent between the JobManager and the TaskManagers. If Flink fails because messages exceed this limit, then you should increase it. The message size requires a size-unit specifier (DEFAULT: <strong>10485760b</strong>).</li>
<li><code>akka.watch.heartbeat.interval</code>: Heartbeat interval for Akka’s DeathWatch mechanism to detect dead TaskManagers. If TaskManagers are wrongly marked dead because of lost or delayed heartbeat messages, then you should increase this value. A thorough description of Akka’s DeathWatch can be found <a href="http://doc.akka.io/docs/akka/snapshot/scala/remoting.html#failure-detector" target="_blank" rel="noopener">here</a> (DEFAULT: <strong>akka.ask.timeout/10</strong>).</li>
<li><code>akka.watch.heartbeat.pause</code>: Acceptable heartbeat pause for Akka’s DeathWatch mechanism. A low value does not allow a irregular heartbeat. A thorough description of Akka’s DeathWatch can be found <a href="http://doc.akka.io/docs/akka/snapshot/scala/remoting.html#failure-detector" target="_blank" rel="noopener">here</a> (DEFAULT: <strong>akka.ask.timeout</strong>).</li>
<li><code>akka.watch.threshold</code>: Threshold for the DeathWatch failure detector. A low value is prone to false positives whereas a high value increases the time to detect a dead TaskManager. A thorough description of Akka’s DeathWatch can be found <a href="http://doc.akka.io/docs/akka/snapshot/scala/remoting.html#failure-detector" target="_blank" rel="noopener">here</a> (DEFAULT: <strong>12</strong>).</li>
<li><code>akka.transport.heartbeat.interval</code>: Heartbeat interval for Akka’s transport failure detector. Since Flink uses TCP, the detector is not necessary. Therefore, the detector is disabled by setting the interval to a very high value. In case you should need the transport failure detector, set the interval to some reasonable value. The interval value requires a time-unit specifier (ms/s/min/h/d) (DEFAULT: <strong>1000 s</strong>).</li>
<li><code>akka.transport.heartbeat.pause</code>: Acceptable heartbeat pause for Akka’s transport failure detector. Since Flink uses TCP, the detector is not necessary. Therefore, the detector is disabled by setting the pause to a very high value. In case you should need the transport failure detector, set the pause to some reasonable value. The pause value requires a time-unit specifier (ms/s/min/h/d) (DEFAULT: <strong>6000 s</strong>).</li>
<li><code>akka.transport.threshold</code>: Threshold for the transport failure detector. Since Flink uses TCP, the detector is not necessary and, thus, the threshold is set to a high value (DEFAULT: <strong>300</strong>).</li>
<li><code>akka.tcp.timeout</code>: Timeout for all outbound connections. If you should experience problems with connecting to a TaskManager due to a slow network, you should increase this value (DEFAULT: <strong>akka.ask.timeout</strong>).</li>
<li><code>akka.throughput</code>: Number of messages that are processed in a batch before returning the thread to the pool. Low values denote a fair scheduling whereas high values can increase the performance at the cost of unfairness (DEFAULT: <strong>15</strong>).</li>
<li><code>akka.log.lifecycle.events</code>: Turns on the Akka’s remote logging of events. Set this value to ‘on’ in case of debugging (DEFAULT: <strong>off</strong>).</li>
<li><code>akka.startup-timeout</code>: Timeout after which the startup of a remote component is considered being failed (DEFAULT: <strong>akka.ask.timeout</strong>).</li>
</ul>
<h2 id="翻译源"><a href="#翻译源" class="headerlink" title="翻译源"></a>翻译源</h2><p><a href="https://cwiki.apache.org/confluence/display/FLINK/Akka+and+Actors" target="_blank" rel="noopener">Akka and Actors</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/11/13/Akka%E5%9C%A8Flink%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/" data-id="ck736a2ps0002ees690zkfe74" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/akka/" rel="tag">akka</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/flink/" rel="tag">flink</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Spring-Boot-Metrics监控之Prometheus-Grafana" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/08/19/Spring-Boot-Metrics%E7%9B%91%E6%8E%A7%E4%B9%8BPrometheus-Grafana/" class="article-date">
  <time datetime="2018-08-19T02:24:23.000Z" itemprop="datePublished">2018-08-19</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/">微服务</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/08/19/Spring-Boot-Metrics%E7%9B%91%E6%8E%A7%E4%B9%8BPrometheus-Grafana/">Spring Boot Metrics监控之Prometheus&amp;Grafana</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>欢迎来到Spring Boot Actuator教程系列的第二部分。在<a href="https://bigjar.github.io/2018/08/19/Spring-Boot-Actuator-%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5%E3%80%81%E5%AE%A1%E8%AE%A1%E3%80%81%E7%BB%9F%E8%AE%A1%E5%92%8C%E7%9B%91%E6%8E%A7/" target="_blank" rel="noopener">第一部分</a>中，你学习到了<code>spring-boot-actuator</code>模块做了什么，如何配置spring boot应用以及如何与各样的actuator endpoints交互。</p>
<p>在这篇文章中，你将学习sprint boot如何整合外部监控系统<a href="https://prometheus.io/" target="_blank" rel="noopener">Prometheus</a>和图表解决方案<a href="https://grafana.com/" target="_blank" rel="noopener">Grafana</a>。</p>
<p>在这篇文章的末尾，你将在自己本地电脑上建立一个Prometheus和Grafana仪表盘，用来可视化监控Spring Boot应用产生的所有metrics。</p>
<h2 id="Prometheus"><a href="#Prometheus" class="headerlink" title="Prometheus"></a>Prometheus</h2><p>Prometheus是一个开源的监控系统，起源于<a href="https://soundcloud.com/" target="_blank" rel="noopener">SoundCloud</a>。它由以下几个核心组件构成：</p>
<ul>
<li>数据爬虫：根据配置的时间定期的通过HTTP抓去metrics数据。</li>
<li><a href="https://en.wikipedia.org/wiki/Time_series" target="_blank" rel="noopener">time-series</a> 数据库：存储所有的metrics数据。</li>
<li>简单的用户交互接口：可视化、查询和监控所有的metrics。</li>
</ul>
<h2 id="Grafana"><a href="#Grafana" class="headerlink" title="Grafana"></a>Grafana</h2><p>Grafana使你能够把来自不同数据源比如Elasticsearch, Prometheus, Graphite, influxDB等多样的数据以绚丽的图标展示出来。</p>
<p>它也能基于你的metrics数据发出告警。当一个告警状态改变时，它能通知你通过email，slack或者其他途径。</p>
<p>值得注意的是，Prometheus仪表盘也有简单的图标。但是Grafana的图表表现的更好。这也是为什么，在这篇文章中，我们将整合Grafana和Pormetheus来可视化metrics数据。</p>
<h2 id="增加Micrometer-Prometheus-Registry到你的Spring-Boot应用"><a href="#增加Micrometer-Prometheus-Registry到你的Spring-Boot应用" class="headerlink" title="增加Micrometer Prometheus Registry到你的Spring Boot应用"></a>增加Micrometer Prometheus Registry到你的Spring Boot应用</h2><p>Spring Boot使用<a href="http://micrometer.io/" target="_blank" rel="noopener">Micrometer</a>，一个应用metrics组件，将actuator metrics整合到外部监控系统中。</p>
<p>它支持很多种监控系统，比如Netflix Atalas, AWS Cloudwatch, Datadog, InfluxData, SignalFx, Graphite, Wavefront和Prometheus等。</p>
<p>为了整合Prometheus，你需要增加<code>micrometer-registry-prometheus</code>依赖：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- Micrometer Prometheus registry  --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">	&lt;groupId&gt;io.micrometer&lt;&#x2F;groupId&gt;</span><br><span class="line">	&lt;artifactId&gt;micrometer-registry-prometheus&lt;&#x2F;artifactId&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>



<p>一旦你增加上述的依赖，Spring Boot会自动配置一个<a href="https://github.com/micrometer-metrics/micrometer/blob/master/implementations/micrometer-registry-prometheus/src/main/java/io/micrometer/prometheus/PrometheusMeterRegistry.java" target="_blank" rel="noopener"><code>PrometheusMeterRegistry</code></a>和<a href="https://github.com/prometheus/client_java/blob/master/simpleclient/src/main/java/io/prometheus/client/CollectorRegistry.java" target="_blank" rel="noopener"><code>CollectorRegistry</code></a>来收集和输出格式化的metrics数据，使得Prometheus服务器可以爬取。</p>
<p>所有应用的metrics数据是根据一个叫<code>/prometheus</code>的endpoint来设置是否可用。Prometheus服务器可以周期性的爬取这个endpoint来获取metrics数据。</p>
<h2 id="解析Spring-Boot-Actuator的-prometheus-Endpoint"><a href="#解析Spring-Boot-Actuator的-prometheus-Endpoint" class="headerlink" title="解析Spring Boot Actuator的/prometheus Endpoint"></a>解析Spring Boot Actuator的/prometheus Endpoint</h2><p>首先，你可以通过actuator endpoint-discovery页面(<a href="http://localhost:8080/actuator" target="_blank" rel="noopener">http://localhost:8080/actuator</a>)来看一下<code>prometheus</code> endpoint。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&quot;prometheus&quot;: &#123;</span><br><span class="line">&quot;href&quot;: &quot;http:&#x2F;&#x2F;127.0.0.1:8080&#x2F;actuator&#x2F;prometheus&quot;,</span><br><span class="line">&quot;templated&quot;: false</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>prometheus</code> endpoint暴露了格式化的metrics数据给Prometheus服务器。你可以通过<code>prometheus</code> endpoint(<a href="http://localhost:8080/actuator/prometheus" target="_blank" rel="noopener">http://localhost:8080/actuator/prometheus</a>)看到被暴露的metrics数据:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># HELP jvm_memory_committed_bytes The amount of memory in bytes that is committed for  the Java virtual machine to use</span><br><span class="line"># TYPE jvm_memory_committed_bytes gauge</span><br><span class="line">jvm_memory_committed_bytes&#123;area&#x3D;&quot;nonheap&quot;,id&#x3D;&quot;Code Cache&quot;,&#125; 9830400.0</span><br><span class="line">jvm_memory_committed_bytes&#123;area&#x3D;&quot;nonheap&quot;,id&#x3D;&quot;Metaspace&quot;,&#125; 4.3032576E7</span><br><span class="line">jvm_memory_committed_bytes&#123;area&#x3D;&quot;nonheap&quot;,id&#x3D;&quot;Compressed Class Space&quot;,&#125; 6070272.0</span><br><span class="line">jvm_memory_committed_bytes&#123;area&#x3D;&quot;heap&quot;,id&#x3D;&quot;PS Eden Space&quot;,&#125; 2.63192576E8</span><br><span class="line">jvm_memory_committed_bytes&#123;area&#x3D;&quot;heap&quot;,id&#x3D;&quot;PS Survivor Space&quot;,&#125; 1.2058624E7</span><br><span class="line">jvm_memory_committed_bytes&#123;area&#x3D;&quot;heap&quot;,id&#x3D;&quot;PS Old Gen&quot;,&#125; 1.96608E8</span><br><span class="line"># HELP logback_events_total Number of error level events that made it to the logs</span><br><span class="line"># TYPE logback_events_total counter</span><br><span class="line">logback_events_total&#123;level&#x3D;&quot;error&quot;,&#125; 0.0</span><br><span class="line">logback_events_total&#123;level&#x3D;&quot;warn&quot;,&#125; 0.0</span><br><span class="line">logback_events_total&#123;level&#x3D;&quot;info&quot;,&#125; 42.0</span><br><span class="line">logback_events_total&#123;level&#x3D;&quot;debug&quot;,&#125; 0.0</span><br><span class="line">logback_events_total&#123;level&#x3D;&quot;trace&quot;,&#125; 0.0</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h2 id="使用Docker下载和运行Prometheus"><a href="#使用Docker下载和运行Prometheus" class="headerlink" title="使用Docker下载和运行Prometheus"></a>使用Docker下载和运行Prometheus</h2><h3 id="下载Prometheus"><a href="#下载Prometheus" class="headerlink" title="下载Prometheus"></a>下载Prometheus</h3><p>你可以使用<code>docker pull</code>命令来下载Prometheus docker image。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker pull prom/prometheus</span></span><br></pre></td></tr></table></figure>

<p>一旦这个image被下载下来，你可以使用<code>docker image ls</code>命令来查看本地的image列表：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker image ls</span></span><br><span class="line">REPOSITORY                                   TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">prom/prometheus                              latest              b82ef1f3aa07        5 days ago          119MB</span><br></pre></td></tr></table></figure>

<h3 id="Prometheus配置-prometheus-yml"><a href="#Prometheus配置-prometheus-yml" class="headerlink" title="Prometheus配置(prometheus.yml)"></a>Prometheus配置(prometheus.yml)</h3><p>接下来，我们需要配置Prometheus来抓取Spring Boot Actuator的<code>/prometheus</code> endpoint中的metrics数据。</p>
<p>创建一个<code>prometheus.yml</code>的文件，填入以下内容：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># my global config</span></span><br><span class="line"><span class="attr">global:</span></span><br><span class="line">  <span class="attr">scrape_interval:</span>     <span class="string">15s</span> <span class="comment"># Set the scrape interval to every 15 seconds. Default is every 1 minute.</span></span><br><span class="line">  <span class="attr">evaluation_interval:</span> <span class="string">15s</span> <span class="comment"># Evaluate rules every 15 seconds. The default is every 1 minute.</span></span><br><span class="line">  <span class="comment"># scrape_timeout is set to the global default (10s).</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load rules once and periodically evaluate them according to the global 'evaluation_interval'.</span></span><br><span class="line"><span class="attr">rule_files:</span></span><br><span class="line">  <span class="comment"># - "first_rules.yml"</span></span><br><span class="line">  <span class="comment"># - "second_rules.yml"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># A scrape configuration containing exactly one endpoint to scrape:</span></span><br><span class="line"><span class="comment"># Here it's Prometheus itself.</span></span><br><span class="line"><span class="attr">scrape_configs:</span></span><br><span class="line">  <span class="comment"># The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">'prometheus'</span></span><br><span class="line">    <span class="comment"># metrics_path defaults to '/metrics'</span></span><br><span class="line">    <span class="comment"># scheme defaults to 'http'.</span></span><br><span class="line">    <span class="attr">static_configs:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">targets:</span> <span class="string">['127.0.0.1:9090']</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">'spring-actuator'</span></span><br><span class="line">    <span class="attr">metrics_path:</span> <span class="string">'/actuator/prometheus'</span></span><br><span class="line">    <span class="attr">scrape_interval:</span> <span class="string">5s</span></span><br><span class="line">    <span class="attr">static_configs:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">targets:</span> <span class="string">['HOST_IP:8080']</span></span><br></pre></td></tr></table></figure>

<p>在Prometheus文档中，上面的配置文件是<a href="https://prometheus.io/docs/prometheus/latest/getting_started/#configuring-prometheus-to-monitor-itself" target="_blank" rel="noopener">basic configuration file</a>的扩展。</p>
<p>上面中比较重要的配置项是<code>spring-actuator</code> job中的<code>scrape_configs</code>选项。</p>
<p><code>metrics_path</code>是Actuator中<code>prometheus</code> endpoint中的路径。<code>targes</code>包含了Spring Boot应用的<code>HOST</code>和<code>PORT</code>。</p>
<p>请确保替换<code>HOST_IP</code>为你Spring Boot应用运行的电脑的IP地址。值得注意的是，<code>localhost</code>将不起作用，因为我们将从docker container中连接HOST机器。你必须设置网络IP地址。</p>
<h3 id="使用Docker运行Prometheus"><a href="#使用Docker运行Prometheus" class="headerlink" title="使用Docker运行Prometheus"></a>使用Docker运行Prometheus</h3><p>最后，让我们在Docker中运行Prometheus。使用以下命令来启动一个Prometheus服务器。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -d --name&#x3D;prometheus -p 9090:9090 -v &lt;PATH_TO_prometheus.yml_FILE&gt;:&#x2F;etc&#x2F;prometheus&#x2F;prometheus.yml prom&#x2F;prometheus --config.file&#x3D;&#x2F;etc&#x2F;prometheus&#x2F;prometheus.yml</span><br></pre></td></tr></table></figure>

<p>请确保替换&lt;PATH_TO_prometheus.yml_FILE&gt;为你在上面创建的Prometheus配置文件的保存的路径。</p>
<p>在运行上述命令之后，docker将在container中启动一个Prometheus服务器。你可以通过以下命令看到所有的container：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ docker container ls</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                    NAMES</span><br><span class="line">e036eb20b8ad        prom&#x2F;prometheus     &quot;&#x2F;bin&#x2F;prometheus --c…&quot;   4 minutes ago       Up 4 minutes        0.0.0.0:9090-&gt;9090&#x2F;tcp   prometheus</span><br></pre></td></tr></table></figure>

<h3 id="在Prometheus仪表盘中可视化Spring-Boot-Metrics"><a href="#在Prometheus仪表盘中可视化Spring-Boot-Metrics" class="headerlink" title="在Prometheus仪表盘中可视化Spring Boot Metrics"></a>在Prometheus仪表盘中可视化Spring Boot Metrics</h3><p>你可以通过访问<a href="http://localhost:9090/" target="_blank" rel="noopener">http://localhost:9090</a>访问Prometheus仪表盘。你可以通过Prometheus查询表达式来查询metrics。</p>
<p>下面是一些例子：</p>
<ul>
<li><p>系统CPU使用</p>
<p><img src="system-cpu-usage.png" alt="system-cpu-usage"></p>
</li>
<li><p>API的延迟响应</p>
<p><img src="response-latency.jpg" alt="response-latency"></p>
</li>
</ul>
<p>你可以从Prometheus官方文档中学习更多的 <a href="https://prometheus.io/docs/introduction/first_steps/#using-the-expression-browser" target="_blank" rel="noopener"><code>Prometheus Query Expressions</code></a>。</p>
<h2 id="使用Docker下载和运行Grafana"><a href="#使用Docker下载和运行Grafana" class="headerlink" title="使用Docker下载和运行Grafana"></a>使用Docker下载和运行Grafana</h2><p>使用以下命令可以使Docker下载和运行Grafana：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker run -d --name=grafana -p 3000:3000 grafana/grafana</span></span><br></pre></td></tr></table></figure>

<p>上述命令将在Docker Container中开启一个Grafana，并且使用3000端口在主机上提供服务。</p>
<p>你可以使用<code>docker container ls</code>来查看Docker container列表：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker container ls</span></span><br><span class="line">CONTAINER ID        IMAGE                           COMMAND                  CREATED             STATUS              PORTS                    NAMES</span><br><span class="line">939dd22a7179        quay.io/prometheus/prometheus   "/bin/prometheus --c…"   14 minutes ago      Up 14 minutes       0.0.0.0:9090-&gt;9090/tcp   vigilant_neumann</span><br><span class="line">1f94c46bcf5c        grafana/grafana                 "/run.sh"                22 hours ago        Up 22 hours         0.0.0.0:3000-&gt;3000/tcp   grafana</span><br></pre></td></tr></table></figure>

<p>你可以访问<a href="http://localhost:3000/" target="_blank" rel="noopener">http://localhost:3000</a>，并且使用默认的账户名(admin)密码(admin)来登录Grafana。</p>
<h3 id="配置Grafana导入Prometheus中的metrics数据"><a href="#配置Grafana导入Prometheus中的metrics数据" class="headerlink" title="配置Grafana导入Prometheus中的metrics数据"></a>配置Grafana导入Prometheus中的metrics数据</h3><p>通过以下几步导入Prometheus中的metrics数据并且在Grafana上可视化。</p>
<h4 id="在Grafana上增加Prometheus数据源"><a href="#在Grafana上增加Prometheus数据源" class="headerlink" title="在Grafana上增加Prometheus数据源"></a>在Grafana上增加Prometheus数据源</h4><p><img src="data-source.png" alt="data-source"></p>
<h4 id="建立一个仪表盘图表"><a href="#建立一个仪表盘图表" class="headerlink" title="建立一个仪表盘图表"></a>建立一个仪表盘图表</h4><p><img src="add-dashboard.png" alt="add-dashboard"></p>
<h4 id="添加一个Prometheus查询"><a href="#添加一个Prometheus查询" class="headerlink" title="添加一个Prometheus查询"></a>添加一个Prometheus查询</h4><p><img src="add-cpu-usage.png" alt="add-cpu-usage"></p>
<h4 id="默认的可视化"><a href="#默认的可视化" class="headerlink" title="默认的可视化"></a>默认的可视化</h4><p><img src="dashboard.png" alt="dashboard"></p>
<p>你可以在<a href="https://github.com/bigjar/actuator-demo" target="_blank" rel="noopener">Github</a>上看到完整的Actutator demo应用。</p>
<p>阅读第一部分：<a href="https://bigjar.github.io/2018/08/19/Spring-Boot-Actuator-%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5%E3%80%81%E5%AE%A1%E8%AE%A1%E3%80%81%E7%BB%9F%E8%AE%A1%E5%92%8C%E7%9B%91%E6%8E%A7/" target="_blank" rel="noopener">Spring Boot Actuator:健康检查、审计、统计和监控</a>。</p>
<h2 id="更多阅读资源"><a href="#更多阅读资源" class="headerlink" title="更多阅读资源"></a>更多阅读资源</h2><ul>
<li><a href="https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-metrics.html" target="_blank" rel="noopener">Spring Boot Production-ready Metrics</a></li>
<li><a href="https://prometheus.io/docs/introduction/first_steps/" target="_blank" rel="noopener">First Steps with Prometheus</a></li>
<li><a href="http://docs.grafana.org/features/datasources/prometheus/" target="_blank" rel="noopener">Using Prometheus in Grafana</a></li>
</ul>
<h2 id="翻译源"><a href="#翻译源" class="headerlink" title="翻译源"></a>翻译源</h2><ul>
<li><a href="https://www.callicoder.com/spring-boot-actuator-metrics-monitoring-dashboard-prometheus-grafana/" target="_blank" rel="noopener">Spring Boot Actuator metrics monitoring with Prometheus and Grafana</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/08/19/Spring-Boot-Metrics%E7%9B%91%E6%8E%A7%E4%B9%8BPrometheus-Grafana/" data-id="ck736a2qr001mees68q4z4h0x" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Actuator/" rel="tag">Actuator</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Grafana/" rel="tag">Grafana</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Prometheus/" rel="tag">Prometheus</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spring-Boot/" rel="tag">Spring Boot</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Spring-Boot-Actuator-健康检查、审计、统计和监控" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/08/19/Spring-Boot-Actuator-%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5%E3%80%81%E5%AE%A1%E8%AE%A1%E3%80%81%E7%BB%9F%E8%AE%A1%E5%92%8C%E7%9B%91%E6%8E%A7/" class="article-date">
  <time datetime="2018-08-18T22:07:28.000Z" itemprop="datePublished">2018-08-19</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/">微服务</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/08/19/Spring-Boot-Actuator-%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5%E3%80%81%E5%AE%A1%E8%AE%A1%E3%80%81%E7%BB%9F%E8%AE%A1%E5%92%8C%E7%9B%91%E6%8E%A7/">Spring Boot Actuator:健康检查、审计、统计和监控</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Spring Boot Actuator可以帮助你监控和管理Spring Boot应用，比如健康检查、审计、统计和HTTP追踪等。所有的这些特性可以通过JMX或者HTTP endpoints来获得。</p>
<p>Actuator同时还可以与外部应用监控系统整合，比如 <a href="https://prometheus.io/" target="_blank" rel="noopener">Prometheus</a>, <a href="https://graphiteapp.org/" target="_blank" rel="noopener">Graphite</a>, <a href="https://www.datadoghq.com/" target="_blank" rel="noopener">DataDog</a>, <a href="https://www.influxdata.com/" target="_blank" rel="noopener">Influx</a>, <a href="https://www.wavefront.com/" target="_blank" rel="noopener">Wavefront</a>, <a href="https://newrelic.com/" target="_blank" rel="noopener">New Relic</a>等。这些系统提供了非常好的仪表盘、图标、分析和告警等功能，使得你可以通过统一的接口轻松的监控和管理你的应用。</p>
<p>Actuator使用<a href="http://micrometer.io/" target="_blank" rel="noopener">Micrometer</a>来整合上面提到的外部应用监控系统。这使得只要通过非常小的配置就可以集成任何应用监控系统。</p>
<p>我将把Spring Boot Actuator教程分为两部分：</p>
<ul>
<li>第一部分(本文)教你如何配置Actuator和通过Http endpoints来进入这些特征。</li>
<li>第二部分教你如何整合Actuator和外部应用监控系统。</li>
</ul>
<h2 id="创建一个有Actuator的Spring-Boot工程"><a href="#创建一个有Actuator的Spring-Boot工程" class="headerlink" title="创建一个有Actuator的Spring Boot工程"></a>创建一个有Actuator的Spring Boot工程</h2><p>首先让我们建一个依赖acutator的简单应用。</p>
<p>你可以使用<a href="https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#getting-started-installing-the-cli" target="_blank" rel="noopener">Spring Boot CLI</a>创建应用：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spring init -d=web,actuator -n=actuator actuator</span><br></pre></td></tr></table></figure>

<p>或者，你可以使用<a href="http://start.spring.io/" target="_blank" rel="noopener">Spring Initializr</a>网站来创建应用：</p>
<p>![Spring initializr](Spring initialzr.png)</p>
<h2 id="增加Spring-Boot-Actuator到一个存在的应用"><a href="#增加Spring-Boot-Actuator到一个存在的应用" class="headerlink" title="增加Spring Boot Actuator到一个存在的应用"></a>增加Spring Boot Actuator到一个存在的应用</h2><p>你可以增加<code>spring-boot-actuator</code>模块到一个已经存在的应用，通过使用下面的依赖。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-actuator<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>对于Gradle，依赖如下：</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dependencies &#123;</span><br><span class="line">	compile(<span class="string">"org.springframework.boot:spring-boot-starter-actuator"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="使用Actuator-Endpoints来监控应用"><a href="#使用Actuator-Endpoints来监控应用" class="headerlink" title="使用Actuator Endpoints来监控应用"></a>使用Actuator Endpoints来监控应用</h2><p>Actuator创建了所谓的<strong>endpoint</strong>来暴露HTTP或者JMX来监控和管理应用。</p>
<p>举个例子，有一个叫<code>/health</code>的endpoint，提供了关于应用健康的基础信息。<code>/metrics</code>endpoints展示了几个有用的度量信息，比如JVM内存使用情况、系统CPU使用情况、打开的文件等等。<code>/loggers</code>endpoint展示了应用的日志和可以让你在运行时改变日志等级。</p>
<p><strong>值得注意的是，每一给actuator endpoint可以被显式的打开和关闭。此外，这些endpoints也需要通过HTTP或者JMX暴露出来，使得它们能被远程进入。</strong></p>
<p>让我们运行应用并且尝试进入默认通过HTTP暴露的打开状态的actuator endpoints。之后，我们将学习如何打开更多的endpoints并且通过HTTP暴露它们。</p>
<p>在应用的根目录下打开命令行工具运行以下命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn spring-boot:run</span><br></pre></td></tr></table></figure>

<p>应用默认使用<code>8080</code>端口运行。一旦这个应用启动了，你可以通过<a href="http://localhost:8080/actuator" target="_blank" rel="noopener">http://localhost:8080/actuator</a>来展示所有通过HTTP暴露的endpoints。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"_links"</span>:&#123;<span class="attr">"self"</span>:&#123;<span class="attr">"href"</span>:<span class="string">"http://localhost:8080/actuator"</span>,<span class="attr">"templated"</span>:<span class="literal">false</span>&#125;,<span class="attr">"health"</span>:&#123;<span class="attr">"href"</span>:<span class="string">"http://localhost:8080/actuator/health"</span>,<span class="attr">"templated"</span>:<span class="literal">false</span>&#125;,<span class="attr">"info"</span>:&#123;<span class="attr">"href"</span>:<span class="string">"http://localhost:8080/actuator/info"</span>,<span class="attr">"templated"</span>:<span class="literal">false</span>&#125;&#125;&#125;</span><br></pre></td></tr></table></figure>

<p>打开<a href="http://localhost:8080/actuator/health" target="_blank" rel="noopener">http://localhost:8080/actuator/health</a>，则会显示如下内容:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;status&quot;:&quot;UP&quot;&#125;</span><br></pre></td></tr></table></figure>

<p>状态将是<code>UP</code>只要应用是健康的。如果应用不健康将会显示<code>DOWN</code>,比如与仪表盘的连接异常或者缺水磁盘空间等。下一节我们将学习spring boot如何决定应用的健康和如何修复这些健康问题。</p>
<p><code>info</code>endpoint(<a href="http://localhost:8080/actuator/info" target="_blank" rel="noopener">http://localhost:8080/actuator/info</a>)展示了关于应用的一般信息，这些信息从编译文件比如<code>META-INF/build-info.properties</code>或者Git文件比如<code>git.properties</code>或者任何环境的property中获取。你将在下一节中学习如何改变这个endpoint的输出。</p>
<p><strong>默认，只有<code>health</code>和<code>info</code>通过HTTP暴露了出来</strong>。这也是为什么<code>/actuator</code>页面只展示了<code>health</code>和<code>info</code>endpoints。我们将学习如何暴露其他的endpoint。首先，让我们看看其他的endpoints是什么。</p>
<p>以下是一些非常有用的actuator endpoints列表。你可以在<a href="https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-endpoints.html" target="_blank" rel="noopener">official documentation</a>上面看到完整的列表。</p>
<table>
<thead>
<tr>
<th>Endpoint ID</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>auditevents</td>
<td>显示应用暴露的审计事件 (比如认证进入、订单失败)</td>
</tr>
<tr>
<td>info</td>
<td>显示应用的基本信息</td>
</tr>
<tr>
<td>health</td>
<td>显示应用的健康状态</td>
</tr>
<tr>
<td>metrics</td>
<td>显示应用多样的度量信息</td>
</tr>
<tr>
<td>loggers</td>
<td>显示和修改配置的loggers</td>
</tr>
<tr>
<td>logfile</td>
<td>返回log file中的内容(如果logging.file或者logging.path被设置)</td>
</tr>
<tr>
<td>httptrace</td>
<td>显示HTTP足迹，最近100个HTTP request/repsponse</td>
</tr>
<tr>
<td>env</td>
<td>显示当前的环境特性</td>
</tr>
<tr>
<td>flyway</td>
<td>显示数据库迁移路径的详细信息</td>
</tr>
<tr>
<td>liquidbase</td>
<td>显示Liquibase 数据库迁移的纤细信息</td>
</tr>
<tr>
<td>shutdown</td>
<td>让你逐步关闭应用</td>
</tr>
<tr>
<td>mappings</td>
<td>显示所有的@RequestMapping路径</td>
</tr>
<tr>
<td>scheduledtasks</td>
<td>显示应用中的调度任务</td>
</tr>
<tr>
<td>threaddump</td>
<td>执行一个线程dump</td>
</tr>
<tr>
<td>heapdump</td>
<td>返回一个GZip压缩的JVM堆dump</td>
</tr>
</tbody></table>
<h2 id="打开和关闭Actuator-Endpoints"><a href="#打开和关闭Actuator-Endpoints" class="headerlink" title="打开和关闭Actuator Endpoints"></a>打开和关闭Actuator Endpoints</h2><p>默认，上述所有的endpints都是打开的，除了<code>shutdown</code> endpoint。</p>
<p>你可以通过设置<code>management.endpoint.&lt;id&gt;.enabled to true or false</code>(<code>id</code>是endpoint的id)来决定打开还是关闭一个actuator endpoint。</p>
<p>举个例子，要想打开<code>shutdown</code> endpoint，增加以下内容在你的<code>application.properties</code>文件中：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">management.endpoint.shutdown.enabled=true</span><br></pre></td></tr></table></figure>



<h2 id="暴露Actuator-Endpoints"><a href="#暴露Actuator-Endpoints" class="headerlink" title="暴露Actuator Endpoints"></a>暴露Actuator Endpoints</h2><p>默认，素偶偶的actuator endpoint通过JMX被暴露，而通过HTTP暴露的只有<code>health</code>和<code>info</code>。</p>
<p>以下是你可以通过应用的properties可以通过HTTP和JMX暴露的actuator endpoint。</p>
<ul>
<li><p>通过HTTP暴露Actuator endpoints。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Use "*" to expose all endpoints, or a comma-separated list to expose selected ones</span><br><span class="line">management.endpoints.web.exposure.include=health,info </span><br><span class="line">management.endpoints.web.exposure.exclude=</span><br></pre></td></tr></table></figure>
</li>
<li><p>通过JMX暴露Actuator endpoints。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Use "*" to expose all endpoints, or a comma-separated list to expose selected ones</span><br><span class="line">management.endpoints.jmx.exposure.include=*</span><br><span class="line">management.endpoints.jmx.exposure.exclude=</span><br></pre></td></tr></table></figure>



</li>
</ul>
<p>通过设置<code>management.endpoints.web.exposure.include</code>为<code>*</code>，我们可以在<a href="http://localhost:8080/actuator" target="_blank" rel="noopener">http://localhost:8080/actuator</a>页面看到如下内容。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"_links"</span>:&#123;<span class="attr">"self"</span>:&#123;<span class="attr">"href"</span>:<span class="string">"http://localhost:8080/actuator"</span>,<span class="attr">"templated"</span>:<span class="literal">false</span>&#125;,<span class="attr">"auditevents"</span>:&#123;<span class="attr">"href"</span>:<span class="string">"http://localhost:8080/actuator/auditevents"</span>,<span class="attr">"templated"</span>:<span class="literal">false</span>&#125;,<span class="attr">"beans"</span>:&#123;<span class="attr">"href"</span>:<span class="string">"http://localhost:8080/actuator/beans"</span>,<span class="attr">"templated"</span>:<span class="literal">false</span>&#125;,<span class="attr">"health"</span>:&#123;<span class="attr">"href"</span>:<span class="string">"http://localhost:8080/actuator/health"</span>,<span class="attr">"templated"</span>:<span class="literal">false</span>&#125;,<span class="attr">"conditions"</span>:&#123;<span class="attr">"href"</span>:<span class="string">"http://localhost:8080/actuator/conditions"</span>,<span class="attr">"templated"</span>:<span class="literal">false</span>&#125;,<span class="attr">"configprops"</span>:&#123;<span class="attr">"href"</span>:<span class="string">"http://localhost:8080/actuator/configprops"</span>,<span class="attr">"templated"</span>:<span class="literal">false</span>&#125;,<span class="attr">"env"</span>:&#123;<span class="attr">"href"</span>:<span class="string">"http://localhost:8080/actuator/env"</span>,<span class="attr">"templated"</span>:<span class="literal">false</span>&#125;,<span class="attr">"env-toMatch"</span>:&#123;<span class="attr">"href"</span>:<span class="string">"http://localhost:8080/actuator/env/&#123;toMatch&#125;"</span>,<span class="attr">"templated"</span>:<span class="literal">true</span>&#125;,<span class="attr">"info"</span>:&#123;<span class="attr">"href"</span>:<span class="string">"http://localhost:8080/actuator/info"</span>,<span class="attr">"templated"</span>:<span class="literal">false</span>&#125;,<span class="attr">"loggers"</span>:&#123;<span class="attr">"href"</span>:<span class="string">"http://localhost:8080/actuator/loggers"</span>,<span class="attr">"templated"</span>:<span class="literal">false</span>&#125;,<span class="attr">"loggers-name"</span>:&#123;<span class="attr">"href"</span>:<span class="string">"http://localhost:8080/actuator/loggers/&#123;name&#125;"</span>,<span class="attr">"templated"</span>:<span class="literal">true</span>&#125;,<span class="attr">"heapdump"</span>:&#123;<span class="attr">"href"</span>:<span class="string">"http://localhost:8080/actuator/heapdump"</span>,<span class="attr">"templated"</span>:<span class="literal">false</span>&#125;,<span class="attr">"threaddump"</span>:&#123;<span class="attr">"href"</span>:<span class="string">"http://localhost:8080/actuator/threaddump"</span>,<span class="attr">"templated"</span>:<span class="literal">false</span>&#125;,<span class="attr">"prometheus"</span>:&#123;<span class="attr">"href"</span>:<span class="string">"http://localhost:8080/actuator/prometheus"</span>,<span class="attr">"templated"</span>:<span class="literal">false</span>&#125;,<span class="attr">"metrics-requiredMetricName"</span>:&#123;<span class="attr">"href"</span>:<span class="string">"http://localhost:8080/actuator/metrics/&#123;requiredMetricName&#125;"</span>,<span class="attr">"templated"</span>:<span class="literal">true</span>&#125;,<span class="attr">"metrics"</span>:&#123;<span class="attr">"href"</span>:<span class="string">"http://localhost:8080/actuator/metrics"</span>,<span class="attr">"templated"</span>:<span class="literal">false</span>&#125;,<span class="attr">"scheduledtasks"</span>:&#123;<span class="attr">"href"</span>:<span class="string">"http://localhost:8080/actuator/scheduledtasks"</span>,<span class="attr">"templated"</span>:<span class="literal">false</span>&#125;,<span class="attr">"httptrace"</span>:&#123;<span class="attr">"href"</span>:<span class="string">"http://localhost:8080/actuator/httptrace"</span>,<span class="attr">"templated"</span>:<span class="literal">false</span>&#125;,<span class="attr">"mappings"</span>:&#123;<span class="attr">"href"</span>:<span class="string">"http://localhost:8080/actuator/mappings"</span>,<span class="attr">"templated"</span>:<span class="literal">false</span>&#125;&#125;&#125;</span><br></pre></td></tr></table></figure>

<h2 id="解析常用的actuator-endpoint"><a href="#解析常用的actuator-endpoint" class="headerlink" title="解析常用的actuator endpoint"></a>解析常用的actuator endpoint</h2><h3 id="health-endpoint"><a href="#health-endpoint" class="headerlink" title="/health endpoint"></a>/health endpoint</h3><p><code>health</code> endpoint通过合并几个健康指数检查应用的健康情况。</p>
<p>Spring Boot Actuator有几个预定义的健康指标比如<a href="https://docs.spring.io/spring-boot/docs/current/api/org/springframework/boot/actuate/jdbc/DataSourceHealthIndicator.html" target="_blank" rel="noopener"><code>DataSourceHealthIndicator</code></a>, <a href="https://docs.spring.io/spring-boot/docs/current/api/org/springframework/boot/actuate/system/DiskSpaceHealthIndicator.html" target="_blank" rel="noopener"><code>DiskSpaceHealthIndicator</code></a>, <a href="https://docs.spring.io/spring-boot/docs/current/api/org/springframework/boot/actuate/mongo/MongoHealthIndicator.html" target="_blank" rel="noopener"><code>MongoHealthIndicator</code></a>, <a href="https://docs.spring.io/spring-boot/docs/current/api/org/springframework/boot/actuate/redis/RedisHealthIndicator.html" target="_blank" rel="noopener"><code>RedisHealthIndicator</code></a>, <a href="https://docs.spring.io/spring-boot/docs/current/api/org/springframework/boot/actuate/cassandra/CassandraHealthIndicator.html" target="_blank" rel="noopener"><code>CassandraHealthIndicator</code></a>等。它使用这些健康指标作为健康检查的一部分。</p>
<p>举个例子，如果你的应用使用<code>Redis</code>，<code>RedisHealthindicator</code>将被当作检查的一部分。如果使用<code>MongoDB</code>，那么<code>MongoHealthIndicator</code>将被当作检查的一部分。</p>
<p>你也可以关闭特定的健康检查指标，比如在prpperties中使用如下命令：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">management.health.mongo.enabled=false</span><br></pre></td></tr></table></figure>

<p>默认，所有的这些健康指标被当作健康检查的一部分。</p>
<h4 id="显示详细的健康信息"><a href="#显示详细的健康信息" class="headerlink" title="显示详细的健康信息"></a>显示详细的健康信息</h4><p><code>health</code> endpoint只展示了简单的<code>UP</code>和<code>DOWN</code>状态。为了获得健康检查中所有指标的详细信息，你可以通过在<code>application.yaml</code>中增加如下内容：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">management:</span><br><span class="line">  endpoint:</span><br><span class="line">    health:</span><br><span class="line">      show-details: always</span><br></pre></td></tr></table></figure>

<p>一旦你打开上述开关，你在<code>/health</code>中可以看到如下详细内容：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"status"</span>:<span class="string">"UP"</span>,<span class="attr">"details"</span>:&#123;<span class="attr">"diskSpace"</span>:&#123;<span class="attr">"status"</span>:<span class="string">"UP"</span>,<span class="attr">"details"</span>:&#123;<span class="attr">"total"</span>:<span class="number">250790436864</span>,<span class="attr">"free"</span>:<span class="number">27172782080</span>,<span class="attr">"threshold"</span>:<span class="number">10485760</span>&#125;&#125;&#125;&#125;</span><br></pre></td></tr></table></figure>

<p><code>health</code> endpoint现在包含了<code>DiskSpaceHealthIndicator</code>。</p>
<p>如果你的应用包含database(比如MySQL)，<code>health</code> endpoint将显示如下内容：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">   <span class="attr">"status"</span>:<span class="string">"UP"</span>,</span><br><span class="line">   <span class="attr">"details"</span>:&#123;</span><br><span class="line">      <span class="attr">"db"</span>:&#123;</span><br><span class="line">         <span class="attr">"status"</span>:<span class="string">"UP"</span>,</span><br><span class="line">         <span class="attr">"details"</span>:&#123;</span><br><span class="line">            <span class="attr">"database"</span>:<span class="string">"MySQL"</span>,</span><br><span class="line">            <span class="attr">"hello"</span>:<span class="number">1</span></span><br><span class="line">         &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">"diskSpace"</span>:&#123;</span><br><span class="line">         <span class="attr">"status"</span>:<span class="string">"UP"</span>,</span><br><span class="line">         <span class="attr">"details"</span>:&#123;</span><br><span class="line">            <span class="attr">"total"</span>:<span class="number">250790436864</span>,</span><br><span class="line">            <span class="attr">"free"</span>:<span class="number">100330897408</span>,</span><br><span class="line">            <span class="attr">"threshold"</span>:<span class="number">10485760</span></span><br><span class="line">         &#125;</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如果你的MySQL server没有启起来，状态将会变成<code>DOWN</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">   &quot;status&quot;:&quot;DOWN&quot;,</span><br><span class="line">   &quot;details&quot;:&#123;</span><br><span class="line">      &quot;db&quot;:&#123;</span><br><span class="line">         &quot;status&quot;:&quot;DOWN&quot;,</span><br><span class="line">         &quot;details&quot;:&#123;</span><br><span class="line">            &quot;error&quot;:&quot;org.springframework.jdbc.CannotGetJdbcConnectionException: Failed to obtain JDBC Connection; nested exception is java.sql.SQLTransientConnectionException: HikariPool-1 - Connection is not available, request timed out after 30006ms.&quot;</span><br><span class="line">         &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;diskSpace&quot;:&#123;</span><br><span class="line">         &quot;status&quot;:&quot;UP&quot;,</span><br><span class="line">         &quot;details&quot;:&#123;</span><br><span class="line">            &quot;total&quot;:250790436864,</span><br><span class="line">            &quot;free&quot;:100324585472,</span><br><span class="line">            &quot;threshold&quot;:10485760</span><br><span class="line">         &#125;</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="创建一个自定义的健康指标"><a href="#创建一个自定义的健康指标" class="headerlink" title="创建一个自定义的健康指标"></a>创建一个自定义的健康指标</h4><p>你可以通过实现<code>HealthIndicator</code>接口来自定义一个健康指标，或者继承<code>AbstractHealthIndicator</code>类。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">package com.example.actuator.health;</span><br><span class="line"></span><br><span class="line">import org.springframework.boot.actuate.health.AbstractHealthIndicator;</span><br><span class="line">import org.springframework.boot.actuate.health.Health;</span><br><span class="line">import org.springframework.stereotype.Component;</span><br><span class="line"></span><br><span class="line">@Component</span><br><span class="line">public class CustomHealthIndicator extends AbstractHealthIndicator &#123;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    protected void doHealthCheck(Health.Builder builder) throws Exception &#123;</span><br><span class="line">        &#x2F;&#x2F; Use the builder to build the health status details that should be reported.</span><br><span class="line">        &#x2F;&#x2F; If you throw an exception, the status will be DOWN with the exception message.</span><br><span class="line">        </span><br><span class="line">        builder.up()</span><br><span class="line">                .withDetail(&quot;app&quot;, &quot;Alive and Kicking&quot;)</span><br><span class="line">                .withDetail(&quot;error&quot;, &quot;Nothing! I&#39;m good.&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>一旦你增加上面的健康指标到你的应用中去后，<code>health</code> endpoint将展示如下细节:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">   &quot;status&quot;:&quot;UP&quot;,</span><br><span class="line">   &quot;details&quot;:&#123;</span><br><span class="line">      &quot;custom&quot;:&#123;</span><br><span class="line">         &quot;status&quot;:&quot;UP&quot;,</span><br><span class="line">         &quot;details&quot;:&#123;</span><br><span class="line">            &quot;app&quot;:&quot;Alive and Kicking&quot;,</span><br><span class="line">            &quot;error&quot;:&quot;Nothing! I&#39;m good.&quot;</span><br><span class="line">         &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;diskSpace&quot;:&#123;</span><br><span class="line">         &quot;status&quot;:&quot;UP&quot;,</span><br><span class="line">         &quot;details&quot;:&#123;</span><br><span class="line">            &quot;total&quot;:250790436864,</span><br><span class="line">            &quot;free&quot;:97949245440,</span><br><span class="line">            &quot;threshold&quot;:10485760</span><br><span class="line">         &#125;</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="metrics-endpoint"><a href="#metrics-endpoint" class="headerlink" title="/metrics endpoint"></a>/metrics endpoint</h3><p><code>metrics</code> endpoint展示了你可以追踪的所有的度量。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">	<span class="attr">"names"</span>: [</span><br><span class="line">		<span class="string">"jvm.memory.max"</span>,</span><br><span class="line">		<span class="string">"http.server.requests"</span>,</span><br><span class="line">		<span class="string">"process.files.max"</span>,</span><br><span class="line">		...</span><br><span class="line">		<span class="string">"tomcat.threads.busy"</span>,</span><br><span class="line">		<span class="string">"process.start.time"</span>,</span><br><span class="line">		<span class="string">"tomcat.servlet.error"</span></span><br><span class="line">	]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>想要获得每个度量的详细信息，你需要传递度量的名称到URL中，像</p>
<p><a href="http://localhost:8080/actuator/metrics/%7BMetricName" target="_blank" rel="noopener">http://localhost:8080/actuator/metrics/{MetricName}</a></p>
<p>举个例子，获得<code>systems.cpu.usage</code>的详细信息，使用以下URL<a href="http://localhost:8080/actuator/metrics/system.cpu.usage" target="_blank" rel="noopener">http://localhost:8080/actuator/metrics/system.cpu.usage</a>。它将显示如下内容:</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">	<span class="attr">"name"</span>: <span class="string">"system.cpu.usage"</span>,</span><br><span class="line">	<span class="attr">"measurements"</span>: [</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="attr">"statistic"</span>: <span class="string">"VALUE"</span>,</span><br><span class="line">		<span class="attr">"value"</span>: <span class="number">0</span></span><br><span class="line">	&#125;</span><br><span class="line">	],</span><br><span class="line"><span class="attr">"availableTags"</span>: []</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="loggers-endpoint"><a href="#loggers-endpoint" class="headerlink" title="/loggers endpoint"></a>/loggers endpoint</h3><p><code>loggers</code> endpoint，可以通过访问<a href="http://localhost:8080/actuator/loggers" target="_blank" rel="noopener">http://localhost:8080/actuator/loggers</a>来进入。它展示了应用中可配置的loggers的列表和相关的日志等级。</p>
<p>你同样能够使用<a href="http://localhost:8080/actuator/loggers/%7Bname" target="_blank" rel="noopener">http://localhost:8080/actuator/loggers/{name}</a>来展示特定logger的细节。</p>
<p>举个例子，为了获得<code>root</code> logger的细节，你可以使用<a href="http://localhost:8080/actuator/loggers/root" target="_blank" rel="noopener">http://localhost:8080/actuator/loggers/root</a>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">   &quot;configuredLevel&quot;:&quot;INFO&quot;,</span><br><span class="line">   &quot;effectiveLevel&quot;:&quot;INFO&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="在运行时改变日志等级"><a href="#在运行时改变日志等级" class="headerlink" title="在运行时改变日志等级"></a>在运行时改变日志等级</h4><p><code>loggers</code> endpoint也允许你在运行时改变应用的日志等级。</p>
<p>举个例子，为了改变<code>root</code> logger的等级为<code>DEBUG</code> ，发送一个<code>POST</code>请求到<a href="http://localhost:8080/actuator/loggers/root" target="_blank" rel="noopener">http://localhost:8080/actuator/loggers/root</a>，加入如下参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">   &quot;configuredLevel&quot;: &quot;DEBUG&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这个功能对于线上问题的排查非常有用。</p>
<p>同时，你可以通过传递<code>null</code>值给<code>configuredLevel</code>来重置日志等级。</p>
<h3 id="info-endpoint"><a href="#info-endpoint" class="headerlink" title="/info endpoint"></a>/info endpoint</h3><p><code>info</code> endpoint展示了应用的基本信息。它通过<code>META-INF/build-info.properties</code>来获得编译信息，通过<code>git.properties</code>来获得Git信息。它同时可以展示任何其他信息，只要这个环境property中含有<code>info</code>key。</p>
<p>你可以增加properties到<code>application.yaml</code>中，比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># INFO ENDPOINT CONFIGURATION</span><br><span class="line">info:</span><br><span class="line">  app:</span><br><span class="line">    name: @project.name@</span><br><span class="line">    description: @project.description@</span><br><span class="line">    version: @project.version@</span><br><span class="line">    encoding: @project.build.sourceEncoding@</span><br><span class="line">    java:</span><br><span class="line">      version: @java.version@</span><br></pre></td></tr></table></figure>

<p>注意，我使用了Spring Boot的<a href="https://docs.spring.io/spring-boot/docs/current/reference/html/howto-properties-and-configuration.html#howto-automatic-expansion" target="_blank" rel="noopener">Automatic property expansion</a> 特征来扩展来自maven工程的properties。</p>
<p>一旦你增加上面的properties，<code>info</code> endpoint将展示如下信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">	&quot;app&quot;: &#123;</span><br><span class="line">	&quot;name&quot;: &quot;actuator&quot;,</span><br><span class="line">	&quot;description&quot;: &quot;Demo project for Spring Boot&quot;,</span><br><span class="line">	&quot;version&quot;: &quot;0.0.1-SNAPSHOT&quot;,</span><br><span class="line">	&quot;encoding&quot;: &quot;UTF-8&quot;,</span><br><span class="line">	&quot;java&quot;: &#123;</span><br><span class="line">		&quot;version&quot;: &quot;1.8.0_161&quot;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="使用Spring-Security来保证Actuator-Endpoints安全"><a href="#使用Spring-Security来保证Actuator-Endpoints安全" class="headerlink" title="使用Spring Security来保证Actuator Endpoints安全"></a>使用Spring Security来保证Actuator Endpoints安全</h2><p>Actuator endpoints是敏感的，必须保障进入是被授权的。如果Spring Security是包含在你的应用中，那么endpoint是通过HTTP认证被保护起来的。</p>
<p>如果没有， 你可以增加以下以来到你的应用中去：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">   &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt;</span><br><span class="line">   &lt;artifactId&gt;spring-boot-starter-security&lt;&#x2F;artifactId&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>

<p>接下去让我们看一下如何覆写spring security配置，并且定义你自己的进入规则。</p>
<p>下面的例子展示了一个简单的spring securiy配置。它使用叫做<a href="https://docs.spring.io/spring-boot/docs/current/api/org/springframework/boot/actuate/autoconfigure/security/servlet/EndpointRequest.html" target="_blank" rel="noopener"><code>EndPointRequest</code></a></p>
<p>的<code>ReqeustMatcher</code>工厂模式来配置Actuator endpoints进入规则。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">package com.example.actuator.config;</span><br><span class="line"></span><br><span class="line">import org.springframework.boot.actuate.autoconfigure.security.servlet.EndpointRequest;</span><br><span class="line">import org.springframework.boot.actuate.context.ShutdownEndpoint;</span><br><span class="line">import org.springframework.boot.autoconfigure.security.servlet.PathRequest;</span><br><span class="line">import org.springframework.context.annotation.Configuration;</span><br><span class="line">import org.springframework.security.config.annotation.web.builders.HttpSecurity;</span><br><span class="line">import org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;</span><br><span class="line"></span><br><span class="line">@Configuration</span><br><span class="line">public class ActuatorSecurityConfig extends WebSecurityConfigurerAdapter &#123;</span><br><span class="line"></span><br><span class="line">    &#x2F;*</span><br><span class="line">        This spring security configuration does the following</span><br><span class="line"></span><br><span class="line">        1. Restrict access to the Shutdown endpoint to the ACTUATOR_ADMIN role.</span><br><span class="line">        2. Allow access to all other actuator endpoints.</span><br><span class="line">        3. Allow access to static resources.</span><br><span class="line">        4. Allow access to the home page (&#x2F;).</span><br><span class="line">        5. All other requests need to be authenticated.</span><br><span class="line">        5. Enable http basic authentication to make the configuration complete.</span><br><span class="line">           You are free to use any other form of authentication.</span><br><span class="line">     *&#x2F;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    protected void configure(HttpSecurity http) throws Exception &#123;</span><br><span class="line">        http</span><br><span class="line">                .authorizeRequests()</span><br><span class="line">                    .requestMatchers(EndpointRequest.to(ShutdownEndpoint.class))</span><br><span class="line">                        .hasRole(&quot;ACTUATOR_ADMIN&quot;)</span><br><span class="line">                    .requestMatchers(EndpointRequest.toAnyEndpoint())</span><br><span class="line">                        .permitAll()</span><br><span class="line">                    .requestMatchers(PathRequest.toStaticResources().atCommonLocations())</span><br><span class="line">                        .permitAll()</span><br><span class="line">                    .antMatchers(&quot;&#x2F;&quot;)</span><br><span class="line">                        .permitAll()</span><br><span class="line">                    .antMatchers(&quot;&#x2F;**&quot;)</span><br><span class="line">                        .authenticated()</span><br><span class="line">                .and()</span><br><span class="line">                .httpBasic();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>为了能够测试以上的配置，你可以在<code>application.yaml</code>中增加spring security用户。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># Spring Security Default user name and password</span><br><span class="line">spring:</span><br><span class="line">  security:</span><br><span class="line">    user:</span><br><span class="line">      name: actuator</span><br><span class="line">      password: actuator</span><br><span class="line">      roles: ACTUATOR_ADMIN</span><br></pre></td></tr></table></figure>

<p>你可以在<a href="https://github.com/bigjar/actuator-demo" target="_blank" rel="noopener">Github</a>上看到完整的代码。</p>
<p>下一部分：<a href="https://bigjar.github.io/2018/08/19/Spring-Boot-Metrics%E7%9B%91%E6%8E%A7%E4%B9%8BPrometheus-Grafana/" target="_blank" rel="noopener">Spring Boot Metrics监控之Prometheus&amp;Grafana</a></p>
<h2 id="更多学习资源"><a href="#更多学习资源" class="headerlink" title="更多学习资源"></a>更多学习资源</h2><ul>
<li><a href="https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready.html" target="_blank" rel="noopener">Spring Boot Actuator: Production-ready features</a></li>
<li><a href="https://spring.io/blog/2018/03/16/micrometer-spring-boot-2-s-new-application-metrics-collector" target="_blank" rel="noopener">Micrometer: Spring Boot 2’s new application metrics collector</a></li>
</ul>
<h2 id="翻译源"><a href="#翻译源" class="headerlink" title="翻译源"></a>翻译源</h2><ul>
<li><a href="https://www.callicoder.com/spring-boot-actuator/" target="_blank" rel="noopener">Spring Boot Actuator: Health check, Auditing, Metrics gathering and Monitoring</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/08/19/Spring-Boot-Actuator-%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5%E3%80%81%E5%AE%A1%E8%AE%A1%E3%80%81%E7%BB%9F%E8%AE%A1%E5%92%8C%E7%9B%91%E6%8E%A7/" data-id="ck736a2qo001eees67fkb9efj" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Actuator/" rel="tag">Actuator</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spring-Boot/" rel="tag">Spring Boot</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Kafka之Producer源码" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/08/12/Kafka%E4%B9%8BProducer%E6%BA%90%E7%A0%81/" class="article-date">
  <time datetime="2018-08-12T11:40:50.000Z" itemprop="datePublished">2018-08-12</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/08/12/Kafka%E4%B9%8BProducer%E6%BA%90%E7%A0%81/">Kafka之Producer源码</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Kafka是一个分布式的流处理平台：</p>
<ul>
<li>发布和订阅数据流，类似于消息队列或者企业消息系统</li>
<li>容错方式存储数据流</li>
<li>数据流到即处理</li>
</ul>
<p>Kafka主要用于以下两种类型的应用：</p>
<ul>
<li>建立从系统或者应用中获取可靠实时的数据流管道</li>
<li>建立转换数据流的实时流应用</li>
</ul>
<p>Kafka有以下4个核心API：</p>
<ul>
<li>Producer API发布一个数据流到一个或多个Kafka topic。</li>
<li>Consumer API订阅一个或多个topic，并且处理topic中的数据。</li>
<li>Streams API作为一个流处理器，消费来自一个或多个topic的输入流，同时产生输出流到一个或多个topic。</li>
<li>Connector API建立运行一个可复用的生产者或者消费者用来连接存在于应用或者数据系统中的topics。</li>
</ul>
<p><img src="kafka_Intro.png" alt="kafka introduce"></p>
<p>本文主要从源码的角度解析一下Producer。</p>
<h2 id="Producer"><a href="#Producer" class="headerlink" title="Producer"></a>Producer</h2><p>Producer发布数据到指定的topics。Producer主要负责数据被分发到对应topic的哪个分区。最简单的负载均衡是通过轮询来进行分区，也可以通过其他的分区函数(根据数据中的key等)。</p>
<p>下面的代码是通过KafkaTemplate模版建立的一个kafka实例，然后调用了send方法把消息发送到”abc123”这个topic上去。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Autowired</span></span><br><span class="line"><span class="keyword">private</span> KafkaTemplate&lt;String, String&gt; kafkaTemplate;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">send</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    String message = <span class="string">"2018-08-07 08:21:47578|1|18701046390|001003|0|2|NULL|2018-08-07 08:21:47:544|2018-08-07 08:21:47:578|0|10.200.1.85|10.200.1.147:7022|"</span>;</span><br><span class="line">    kafkaTemplate.send(<span class="string">"abc123"</span>, message);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其内部实现主要是依靠doSend方法。首先进来判断是否设置了支持事务，接着获取了一个producer实例，然后调用其send方法。在send的回调结束后调用了closeProducer方法来关闭producer。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"> 	<span class="keyword">protected</span> ListenableFuture&lt;SendResult&lt;K, V&gt;&gt; doSend(<span class="keyword">final</span> ProducerRecord&lt;K, V&gt; producerRecord) &#123;</span><br><span class="line">		<span class="keyword">if</span> (<span class="keyword">this</span>.transactional) &#123;</span><br><span class="line">			Assert.state(inTransaction(),</span><br><span class="line">					<span class="string">"No transaction is in process; "</span></span><br><span class="line">						+ <span class="string">"possible solutions: run the template operation within the scope of a "</span></span><br><span class="line">						+ <span class="string">"template.executeInTransaction() operation, start a transaction with @Transactional "</span></span><br><span class="line">						+ <span class="string">"before invoking the template method, "</span></span><br><span class="line">						+ <span class="string">"run in a transaction started by a listener container when consuming a record"</span>);</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">final</span> Producer&lt;K, V&gt; producer = getTheProducer();</span><br><span class="line">		<span class="keyword">if</span> (<span class="keyword">this</span>.logger.isTraceEnabled()) &#123;</span><br><span class="line">			<span class="keyword">this</span>.logger.trace(<span class="string">"Sending: "</span> + producerRecord);</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">final</span> SettableListenableFuture&lt;SendResult&lt;K, V&gt;&gt; future = <span class="keyword">new</span> SettableListenableFuture&lt;&gt;();</span><br><span class="line">		producer.send(producerRecord, <span class="keyword">new</span> Callback() &#123;</span><br><span class="line"></span><br><span class="line">		<span class="meta">@Override</span></span><br><span class="line">		<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> </span>&#123;</span><br><span class="line">			<span class="keyword">try</span> &#123;</span><br><span class="line">				<span class="keyword">if</span> (exception == <span class="keyword">null</span>) &#123;</span><br><span class="line">					future.set(<span class="keyword">new</span> SendResult&lt;&gt;(producerRecord, metadata));</span><br><span class="line">					<span class="keyword">if</span> (KafkaTemplate.<span class="keyword">this</span>.producerListener != <span class="keyword">null</span>) &#123;</span><br><span class="line">						KafkaTemplate.<span class="keyword">this</span>.producerListener.onSuccess(producerRecord, metadata);</span><br><span class="line">					&#125;</span><br><span class="line">					<span class="keyword">if</span> (KafkaTemplate.<span class="keyword">this</span>.logger.isTraceEnabled()) &#123;</span><br><span class="line">						KafkaTemplate.<span class="keyword">this</span>.logger.trace(<span class="string">"Sent ok: "</span> + producerRecord + <span class="string">", metadata: "</span> + metadata);</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;</span><br><span class="line">				<span class="keyword">else</span> &#123;</span><br><span class="line">					future.setException(<span class="keyword">new</span> KafkaProducerException(producerRecord, <span class="string">"Failed to send"</span>, exception));</span><br><span class="line">					<span class="keyword">if</span> (KafkaTemplate.<span class="keyword">this</span>.producerListener != <span class="keyword">null</span>) &#123;</span><br><span class="line">						KafkaTemplate.<span class="keyword">this</span>.producerListener.onError(producerRecord, exception);</span><br><span class="line">					&#125;</span><br><span class="line">					<span class="keyword">if</span> (KafkaTemplate.<span class="keyword">this</span>.logger.isDebugEnabled()) &#123;</span><br><span class="line">						KafkaTemplate.<span class="keyword">this</span>.logger.debug(<span class="string">"Failed to send: "</span> + producerRecord, exception);</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">			<span class="keyword">finally</span> &#123;</span><br><span class="line">				<span class="keyword">if</span> (!KafkaTemplate.<span class="keyword">this</span>.transactional) &#123;</span><br><span class="line">					closeProducer(producer, <span class="keyword">false</span>);</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">	&#125;);</span><br><span class="line">	<span class="keyword">if</span> (<span class="keyword">this</span>.autoFlush) &#123;</span><br><span class="line">		flush();</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> (<span class="keyword">this</span>.logger.isTraceEnabled()) &#123;</span><br><span class="line">		<span class="keyword">this</span>.logger.trace(<span class="string">"Sent: "</span> + producerRecord);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> future;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>producer中的doSend方法实现异步发送数据到topic。</p>
<ol>
<li>确认topic的元数据是可用的，并设置等待超时时间。</li>
<li>序列化record的key，topic和header。</li>
<li>序列化record的value，topic，header。</li>
<li>设置record的分区。这边如果在最开始传入时设置了分区，就用设置的分区，如果没有，就用轮询的方式计算。</li>
<li>检查序列化后要传输的record是否超过限制；</li>
<li>把前面设置好的分区、序列化的key，value、超时时间、header等参数放入到累加器中。</li>
<li>如果返回的结果显示批队列已经满了或者新建立了一个批队列，那么就唤醒这个sender发送数据。</li>
<li>返回result的future给上层。</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> Future&lt;RecordMetadata&gt; <span class="title">doSend</span><span class="params">(ProducerRecord&lt;K, V&gt; record, Callback callback)</span> </span>&#123;</span><br><span class="line">    TopicPartition tp = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// first make sure the metadata for the topic is available</span></span><br><span class="line">        ClusterAndWaitTime clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), maxBlockTimeMs);</span><br><span class="line">        <span class="keyword">long</span> remainingWaitMs = Math.max(<span class="number">0</span>, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs);</span><br><span class="line">        Cluster cluster = clusterAndWaitTime.cluster;</span><br><span class="line">        <span class="keyword">byte</span>[] serializedKey;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            serializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key());</span><br><span class="line">        &#125; <span class="keyword">catch</span> (ClassCastException cce) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">"Can't convert key of class "</span> + record.key().getClass().getName() +</span><br><span class="line">                    <span class="string">" to class "</span> + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() +</span><br><span class="line">                    <span class="string">" specified in key.serializer"</span>, cce);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">byte</span>[] serializedValue;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            serializedValue = valueSerializer.serialize(record.topic(), record.headers(), record.value());</span><br><span class="line">        &#125; <span class="keyword">catch</span> (ClassCastException cce) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">"Can't convert value of class "</span> + record.value().getClass().getName() +</span><br><span class="line">                    <span class="string">" to class "</span> + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() +</span><br><span class="line">                    <span class="string">" specified in value.serializer"</span>, cce);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> partition = partition(record, serializedKey, serializedValue, cluster);</span><br><span class="line">        tp = <span class="keyword">new</span> TopicPartition(record.topic(), partition);</span><br><span class="line"></span><br><span class="line">        setReadOnly(record.headers());</span><br><span class="line">        Header[] headers = record.headers().toArray();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> serializedSize = AbstractRecords.estimateSizeInBytesUpperBound(apiVersions.maxUsableProduceMagic(),</span><br><span class="line">                compressionType, serializedKey, serializedValue, headers);</span><br><span class="line">        ensureValidRecordSize(serializedSize);</span><br><span class="line">        <span class="keyword">long</span> timestamp = record.timestamp() == <span class="keyword">null</span> ? time.milliseconds() : record.timestamp();</span><br><span class="line">        log.trace(<span class="string">"Sending record &#123;&#125; with callback &#123;&#125; to topic &#123;&#125; partition &#123;&#125;"</span>, record, callback, record.topic(), partition);</span><br><span class="line">        <span class="comment">// producer callback will make sure to call both 'callback' and interceptor callback</span></span><br><span class="line">        Callback interceptCallback = <span class="keyword">this</span>.interceptors == <span class="keyword">null</span> ? callback : <span class="keyword">new</span> InterceptorCallback&lt;&gt;(callback, <span class="keyword">this</span>.interceptors, tp);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (transactionManager != <span class="keyword">null</span> &amp;&amp; transactionManager.isTransactional())</span><br><span class="line">            transactionManager.maybeAddPartitionToTransaction(tp);</span><br><span class="line"></span><br><span class="line">        RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey,</span><br><span class="line">                serializedValue, headers, interceptCallback, remainingWaitMs);</span><br><span class="line">        <span class="keyword">if</span> (result.batchIsFull || result.newBatchCreated) &#123;</span><br><span class="line">            log.trace(<span class="string">"Waking up the sender since topic &#123;&#125; partition &#123;&#125; is either full or getting a new batch"</span>, record.topic(), partition);</span><br><span class="line">            <span class="keyword">this</span>.sender.wakeup();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result.future;</span><br><span class="line">        <span class="comment">// handling exceptions and record the errors;</span></span><br><span class="line">        <span class="comment">// for API exceptions return them in the future,</span></span><br><span class="line">        <span class="comment">// for other exceptions throw directly</span></span><br><span class="line">    &#125; <span class="keyword">catch</span> (ApiException e) &#123;</span><br><span class="line">        log.debug(<span class="string">"Exception occurred during message send:"</span>, e);</span><br><span class="line">        <span class="keyword">if</span> (callback != <span class="keyword">null</span>)</span><br><span class="line">            callback.onCompletion(<span class="keyword">null</span>, e);</span><br><span class="line">        <span class="keyword">this</span>.errors.record();</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.interceptors != <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">this</span>.interceptors.onSendError(record, tp, e);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> FutureFailure(e);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        <span class="keyword">this</span>.errors.record();</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.interceptors != <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">this</span>.interceptors.onSendError(record, tp, e);</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> InterruptException(e);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (BufferExhaustedException e) &#123;</span><br><span class="line">        <span class="keyword">this</span>.errors.record();</span><br><span class="line">        <span class="keyword">this</span>.metrics.sensor(<span class="string">"buffer-exhausted-records"</span>).record();</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.interceptors != <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">this</span>.interceptors.onSendError(record, tp, e);</span><br><span class="line">        <span class="keyword">throw</span> e;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (KafkaException e) &#123;</span><br><span class="line">        <span class="keyword">this</span>.errors.record();</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.interceptors != <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">this</span>.interceptors.onSendError(record, tp, e);</span><br><span class="line">        <span class="keyword">throw</span> e;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        <span class="comment">// we notify interceptor about all exceptions, since onSend is called before anything else in this method</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.interceptors != <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">this</span>.interceptors.onSendError(record, tp, e);</span><br><span class="line">        <span class="keyword">throw</span> e;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>RecordAccumulator中的append方法用于把record添加到累加器中，并返回累加的结果。</p>
<p>首先它检查是否有一个在处理的batch。如果有，直接尝试增加序列化后的record到累加器中。如果没有，则创建一个带有缓冲区的新的batch，然后尝试增加序列化后的record到batch中的缓冲区内，接着增加batch到队列中。最终返回累加的结果。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> RecordAppendResult <span class="title">append</span><span class="params">(TopicPartition tp,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 <span class="keyword">long</span> timestamp,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 <span class="keyword">byte</span>[] key,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 <span class="keyword">byte</span>[] value,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 Header[] headers,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 Callback callback,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 <span class="keyword">long</span> maxTimeToBlock)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// We keep track of the number of appending thread to make sure we do not miss batches in</span></span><br><span class="line">    <span class="comment">// abortIncompleteBatches().</span></span><br><span class="line">    appendsInProgress.incrementAndGet();</span><br><span class="line">    ByteBuffer buffer = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">if</span> (headers == <span class="keyword">null</span>) headers = Record.EMPTY_HEADERS;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// check if we have an in-progress batch</span></span><br><span class="line">        Deque&lt;ProducerBatch&gt; dq = getOrCreateDeque(tp);</span><br><span class="line">        <span class="keyword">synchronized</span> (dq) &#123;</span><br><span class="line">            <span class="keyword">if</span> (closed)</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Cannot send after the producer is closed."</span>);</span><br><span class="line">            RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq);</span><br><span class="line">            <span class="keyword">if</span> (appendResult != <span class="keyword">null</span>)</span><br><span class="line">                <span class="keyword">return</span> appendResult;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// we don't have an in-progress record batch try to allocate a new batch</span></span><br><span class="line">        <span class="keyword">byte</span> maxUsableMagic = apiVersions.maxUsableProduceMagic();</span><br><span class="line">        <span class="keyword">int</span> size = Math.max(<span class="keyword">this</span>.batchSize, AbstractRecords.estimateSizeInBytesUpperBound(maxUsableMagic, compression, key, value, headers));</span><br><span class="line">        log.trace(<span class="string">"Allocating a new &#123;&#125; byte message buffer for topic &#123;&#125; partition &#123;&#125;"</span>, size, tp.topic(), tp.partition());</span><br><span class="line">        buffer = free.allocate(size, maxTimeToBlock);</span><br><span class="line">        <span class="keyword">synchronized</span> (dq) &#123;</span><br><span class="line">            <span class="comment">// Need to check if producer is closed again after grabbing the dequeue lock.</span></span><br><span class="line">            <span class="keyword">if</span> (closed)</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Cannot send after the producer is closed."</span>);</span><br><span class="line"></span><br><span class="line">            RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq);</span><br><span class="line">            <span class="keyword">if</span> (appendResult != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="comment">// Somebody else found us a batch, return the one we waited for! Hopefully this doesn't happen often...</span></span><br><span class="line">                <span class="keyword">return</span> appendResult;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            MemoryRecordsBuilder recordsBuilder = recordsBuilder(buffer, maxUsableMagic);</span><br><span class="line">            ProducerBatch batch = <span class="keyword">new</span> ProducerBatch(tp, recordsBuilder, time.milliseconds());</span><br><span class="line">            FutureRecordMetadata future = Utils.notNull(batch.tryAppend(timestamp, key, value, headers, callback, time.milliseconds()));</span><br><span class="line"></span><br><span class="line">            dq.addLast(batch);</span><br><span class="line">            incomplete.add(batch);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Don't deallocate this buffer in the finally block as it's being used in the record batch</span></span><br><span class="line">            buffer = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> RecordAppendResult(future, dq.size() &gt; <span class="number">1</span> || batch.isFull(), <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (buffer != <span class="keyword">null</span>)</span><br><span class="line">            free.deallocate(buffer);</span><br><span class="line">        appendsInProgress.decrementAndGet();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>真正发送record到集群的的类是Sender类，它是一个bachground thread。它在run方法中调用sendProducerData方法。</p>
<p>而sendProducerData方法做了以下事情：</p>
<ol>
<li>从累加器中获取可以准备发送的record</li>
<li>如果有任何分区的leader还不知道，强制元数据更新</li>
<li>移除还没有准备好发送的节点</li>
<li>创建一个request请求用于发送batch</li>
<li>对于过期的batch进行reset producer id</li>
<li>发送batch request</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">long</span> <span class="title">sendProducerData</span><span class="params">(<span class="keyword">long</span> now)</span> </span>&#123;</span><br><span class="line">    Cluster cluster = metadata.fetch();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// get the list of partitions with data ready to send</span></span><br><span class="line">    RecordAccumulator.ReadyCheckResult result = <span class="keyword">this</span>.accumulator.ready(cluster, now);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// if there are any partitions whose leaders are not known yet, force metadata update</span></span><br><span class="line">    <span class="keyword">if</span> (!result.unknownLeaderTopics.isEmpty()) &#123;</span><br><span class="line">        <span class="comment">// The set of topics with unknown leader contains topics with leader election pending as well as</span></span><br><span class="line">        <span class="comment">// topics which may have expired. Add the topic again to metadata to ensure it is included</span></span><br><span class="line">        <span class="comment">// and request metadata update, since there are messages to send to the topic.</span></span><br><span class="line">        <span class="keyword">for</span> (String topic : result.unknownLeaderTopics)</span><br><span class="line">            <span class="keyword">this</span>.metadata.add(topic);</span><br><span class="line">        <span class="keyword">this</span>.metadata.requestUpdate();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// remove any nodes we aren't ready to send to</span></span><br><span class="line">    Iterator&lt;Node&gt; iter = result.readyNodes.iterator();</span><br><span class="line">    <span class="keyword">long</span> notReadyTimeout = Long.MAX_VALUE;</span><br><span class="line">    <span class="keyword">while</span> (iter.hasNext()) &#123;</span><br><span class="line">        Node node = iter.next();</span><br><span class="line">        <span class="keyword">if</span> (!<span class="keyword">this</span>.client.ready(node, now)) &#123;</span><br><span class="line">            iter.remove();</span><br><span class="line">            notReadyTimeout = Math.min(notReadyTimeout, <span class="keyword">this</span>.client.connectionDelay(node, now));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// create produce requests</span></span><br><span class="line">    Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; batches = <span class="keyword">this</span>.accumulator.drain(cluster, result.readyNodes,</span><br><span class="line">            <span class="keyword">this</span>.maxRequestSize, now);</span><br><span class="line">    <span class="keyword">if</span> (guaranteeMessageOrder) &#123;</span><br><span class="line">        <span class="comment">// Mute all the partitions drained</span></span><br><span class="line">        <span class="keyword">for</span> (List&lt;ProducerBatch&gt; batchList : batches.values()) &#123;</span><br><span class="line">            <span class="keyword">for</span> (ProducerBatch batch : batchList)</span><br><span class="line">                <span class="keyword">this</span>.accumulator.mutePartition(batch.topicPartition);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    List&lt;ProducerBatch&gt; expiredBatches = <span class="keyword">this</span>.accumulator.expiredBatches(<span class="keyword">this</span>.requestTimeout, now);</span><br><span class="line">    <span class="comment">// Reset the producer id if an expired batch has previously been sent to the broker. Also update the metrics</span></span><br><span class="line">    <span class="comment">// for expired batches. see the documentation of @TransactionState.resetProducerId to understand why</span></span><br><span class="line">    <span class="comment">// we need to reset the producer id here.</span></span><br><span class="line">    <span class="keyword">if</span> (!expiredBatches.isEmpty())</span><br><span class="line">        log.trace(<span class="string">"Expired &#123;&#125; batches in accumulator"</span>, expiredBatches.size());</span><br><span class="line">    <span class="keyword">for</span> (ProducerBatch expiredBatch : expiredBatches) &#123;</span><br><span class="line">        failBatch(expiredBatch, -<span class="number">1</span>, NO_TIMESTAMP, expiredBatch.timeoutException(), <span class="keyword">false</span>);</span><br><span class="line">        <span class="keyword">if</span> (transactionManager != <span class="keyword">null</span> &amp;&amp; expiredBatch.inRetry()) &#123;</span><br><span class="line">            <span class="comment">// This ensures that no new batches are drained until the current in flight batches are fully resolved.</span></span><br><span class="line">            transactionManager.markSequenceUnresolved(expiredBatch.topicPartition);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    sensors.updateProduceRequestMetrics(batches);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// If we have any nodes that are ready to send + have sendable data, poll with 0 timeout so this can immediately</span></span><br><span class="line">    <span class="comment">// loop and try sending more data. Otherwise, the timeout is determined by nodes that have partitions with data</span></span><br><span class="line">    <span class="comment">// that isn't yet sendable (e.g. lingering, backing off). Note that this specifically does not include nodes</span></span><br><span class="line">    <span class="comment">// with sendable data that aren't ready to send since they would cause busy looping.</span></span><br><span class="line">    <span class="keyword">long</span> pollTimeout = Math.min(result.nextReadyCheckDelayMs, notReadyTimeout);</span><br><span class="line">    <span class="keyword">if</span> (!result.readyNodes.isEmpty()) &#123;</span><br><span class="line">        log.trace(<span class="string">"Nodes with data ready to send: &#123;&#125;"</span>, result.readyNodes);</span><br><span class="line">        <span class="comment">// if some partitions are already ready to be sent, the select time would be 0;</span></span><br><span class="line">        <span class="comment">// otherwise if some partition already has some data accumulated but not ready yet,</span></span><br><span class="line">        <span class="comment">// the select time will be the time difference between now and its linger expiry time;</span></span><br><span class="line">        <span class="comment">// otherwise the select time will be the time difference between now and the metadata expiry time;</span></span><br><span class="line">        pollTimeout = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    sendProduceRequests(batches, now);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pollTimeout;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文简单介绍了Kafka的基本情况，包含Producer、Consumer、Streams、Connector4个API。接着从源码入手分析了Producer发送数据到集群的过程，其主要是把数据放入缓冲，然后再从缓冲区发送数据。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/08/12/Kafka%E4%B9%8BProducer%E6%BA%90%E7%A0%81/" data-id="ck736a2qk0010ees6gwg3g8ry" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kafka/" rel="tag">kafka</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Flink中的wiki-edits例子实践" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/06/25/Flink%E4%B8%AD%E7%9A%84wiki-edits%E4%BE%8B%E5%AD%90%E5%AE%9E%E8%B7%B5/" class="article-date">
  <time datetime="2018-06-25T06:50:07.000Z" itemprop="datePublished">2018-06-25</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/06/25/Flink%E4%B8%AD%E7%9A%84wiki-edits%E4%BE%8B%E5%AD%90%E5%AE%9E%E8%B7%B5/">Flink中的wiki-edits例子实践</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><p>wiki-edits教程是一个监控wikipedia编辑的flink监控程序，实时计算编辑者的编辑的byte数。它通过wikipedia connector来获取数据源，最终把数据sink到kafka中。</p>
<h2 id="建立Maven工程"><a href="#建立Maven工程" class="headerlink" title="建立Maven工程"></a>建立Maven工程</h2><p>我们使用Flink的Maven原型来创建工程。Flink的版本号为1.5.0，脚本命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ mvn archetype:generate \</span><br><span class="line">    -DarchetypeGroupId&#x3D;org.apache.flink \</span><br><span class="line">    -DarchetypeArtifactId&#x3D;flink-quickstart-java \</span><br><span class="line">    -DarchetypeVersion&#x3D;1.5.0 \</span><br><span class="line">    -DgroupId&#x3D;wiki-edits \</span><br><span class="line">    -DartifactId&#x3D;wiki-edits \</span><br><span class="line">    -Dversion&#x3D;0.1 \</span><br><span class="line">    -Dpackage&#x3D;wikiedits \</span><br><span class="line">    -DinteractiveMode&#x3D;false</span><br></pre></td></tr></table></figure>
<p>然后我们可以通过tree命令来查看目录结构。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ wiki-edits&#x2F;</span><br><span class="line">├── pom.xml</span><br><span class="line">├── src</span><br><span class="line">│   └── main</span><br><span class="line">│       ├── java</span><br><span class="line">│       │   └── wikiedits</span><br><span class="line">│       │       ├── BatchJob.java</span><br><span class="line">│       │       ├── StreamingJob.java</span><br><span class="line">│       └── resources</span><br><span class="line">│           └── log4j.properties</span><br></pre></td></tr></table></figure>
<p>最后我们用IDEA打开工程，并在pom.xml中添加如下依赖，分别为对flink-connector-wikiedits和flink-connector-kafka的依赖。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">	&lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">	&lt;artifactId&gt;flink-connector-wikiedits_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">	&lt;version&gt;$&#123;flink.version&#125;&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">	&lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">	&lt;artifactId&gt;flink-connector-kafka-0.8_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">	&lt;version&gt;$&#123;flink.version&#125;&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>
<h2 id="编写Flink程序"><a href="#编写Flink程序" class="headerlink" title="编写Flink程序"></a>编写Flink程序</h2><p>首先我们创建一个WikipediaAnalysis.java文件，并在main方法中添加如下代码。其大致步骤分为如下：</p>
<ol>
<li>获取环境信息</li>
<li>为环境信息添加WikipediaEditsSource源</li>
<li>根据事件中的用户名为key来区分数据流</li>
<li>设置窗口时间为5s</li>
<li>聚合当前窗口中相同用户名的事件，最终返回一个tuple2&lt;user，累加的ByteDiff&gt;</li>
<li>把tuple2映射为string</li>
<li>sink数据到kafka，topic为wiki-result</li>
<li>执行操作</li>
</ol>
<blockquote>
<p>keyBy(…)函数是用来分片数据源的，可以把相同key的放在一个task任务中执行。</p>
</blockquote>
<blockquote>
<p>timeWindow(…)函数默认使用tumbling windows。</p>
</blockquote>
<blockquote>
<p>这边聚合函数使用了Aggregation函数，替换了原先的fold函数(提示为deprecated)。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> wikiedits;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.AggregateFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.functions.KeySelector;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.KeyedStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer08;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.wikiedits.WikipediaEditEvent;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.wikiedits.WikipediaEditsSource;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WikipediaAnalysis</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="comment">//1.获取环境信息</span></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2.为环境信息添加WikipediaEditsSource源</span></span><br><span class="line">        DataStream&lt;WikipediaEditEvent&gt; edits = env.addSource(<span class="keyword">new</span> WikipediaEditsSource());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.根据事件中的用户名为key来区分数据流</span></span><br><span class="line">        KeyedStream&lt;WikipediaEditEvent, String&gt; keyedEdits = edits</span><br><span class="line">                .keyBy(<span class="keyword">new</span> KeySelector&lt;WikipediaEditEvent, String&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> String <span class="title">getKey</span><span class="params">(WikipediaEditEvent wikipediaEditEvent)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> wikipediaEditEvent.getUser();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; result = keyedEdits</span><br><span class="line">                <span class="comment">//4.设置窗口时间为5s</span></span><br><span class="line">                .timeWindow(Time.seconds(<span class="number">5</span>))</span><br><span class="line">                <span class="comment">//5.聚合当前窗口中相同用户名的事件，最终返回一个tuple2&lt;user，累加的ByteDiff&gt;</span></span><br><span class="line">                .aggregate(<span class="keyword">new</span> AggregateFunction&lt;WikipediaEditEvent, Tuple2&lt;String, Integer&gt;, Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">""</span>,<span class="number">0</span>);</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">add</span><span class="params">(WikipediaEditEvent value, Tuple2&lt;String, Integer&gt; accumulator)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(value.getUser(), value.getByteDiff()+accumulator.f1);</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">getResult</span><span class="params">(Tuple2&lt;String, Integer&gt; accumulator)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> accumulator;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">merge</span><span class="params">(Tuple2&lt;String, Integer&gt; a, Tuple2&lt;String, Integer&gt; b)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(a.f0+b.f0, a.f1+b.f1);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//6.把tuple2映射为string</span></span><br><span class="line">        result.map(<span class="keyword">new</span> MapFunction&lt;Tuple2&lt;String,Integer&gt;, String&gt;() &#123;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(Tuple2&lt;String, Integer&gt; stringLongTuple2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> stringLongTuple2.toString();</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//7.sink数据到kafka，topic为wiki-result</span></span><br><span class="line">        &#125;).addSink(<span class="keyword">new</span> FlinkKafkaProducer08&lt;String&gt;(<span class="string">"localhost:9092"</span>, <span class="string">"wiki-result"</span>, <span class="keyword">new</span> SimpleStringSchema()));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//8.执行操作</span></span><br><span class="line">        env.execute();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>最后我们添加一下IDEA的运行配置信息。</p>
<p><img src="idea-config.png" alt="配置"></p>
<h3 id="安装运行zookeeper-amp-kafka"><a href="#安装运行zookeeper-amp-kafka" class="headerlink" title="安装运行zookeeper&amp;kafka"></a>安装运行zookeeper&amp;kafka</h3><p>Mac可以通过brew来安装<a href="https://zookeeper.apache.org/" target="_blank" rel="noopener">zookeeper</a>和<a href="https://kafka.apache.org/" target="_blank" rel="noopener">kafka</a>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ brew install zookeeper</span><br><span class="line">$ brew install kafka</span><br></pre></td></tr></table></figure>
<p>然后运行上述组件。在zookeeper目录下执行以下命令来zookeeper开启服务。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;zkServer start</span><br></pre></td></tr></table></figure>

<p>在kafka目录下执行以下命令来开启kafka服务。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;kafka-server-start &#x2F;usr&#x2F;local&#x2F;etc&#x2F;kafka&#x2F;server.properties</span><br></pre></td></tr></table></figure>

<p>接着创建一个topic。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;kafka-console-producer --topic wiki-result  --broker-list localhost:9092</span><br></pre></td></tr></table></figure>

<h3 id="运行程序并消费kafka中的数据"><a href="#运行程序并消费kafka中的数据" class="headerlink" title="运行程序并消费kafka中的数据"></a>运行程序并消费kafka中的数据</h3><p>在IDEA中run刚才的程序，然后在kafka目录中执行开启消费者的命令，可以查看实时消费的数据。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;kafka-console-consumer --zookeeper localhost:2181 --topic wiki-result</span><br><span class="line">(Tony1,17)</span><br><span class="line">(2.177.40.137,9)</span><br><span class="line">(Waelabdelhamid,279)</span><br><span class="line">(Falconatic,182)</span><br><span class="line">(JackintheBox,1934)</span><br><span class="line">(Zzbrandon123,26)</span><br><span class="line">(0.86.42.171,56)</span><br><span class="line">(.37.168.68,-44)</span><br><span class="line">(Aditya debnath wiki,3)</span><br></pre></td></tr></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文实践了Flink的wiki-edit例子。其通过从wiki-connector中获取source，并sink数据到kafka中。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.5/quickstart/run_example_quickstart.html" target="_blank" rel="noopener">Monitoring the Wikipedia Edit Stream</a><br><a href="https://kafka.apache.org/" target="_blank" rel="noopener">kafka</a><br><a href="https://zookeeper.apache.org/" target="_blank" rel="noopener">zookeeper</a><br><a href="https://stackoverflow.com/questions/47123785/flink-how-to-convert-the-deprecated-fold-to-aggregrate" target="_blank" rel="noopener">Flink: How to convert the deprecated fold to aggregrate?</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/06/25/Flink%E4%B8%AD%E7%9A%84wiki-edits%E4%BE%8B%E5%AD%90%E5%AE%9E%E8%B7%B5/" data-id="ck736a2q5000dees6ajpc9crf" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/flink/" rel="tag">flink</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kafka/" rel="tag">kafka</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Flink在Mac-OS-X上的安装与启动" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/06/23/Flink%E5%9C%A8Mac-OS-X%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E5%90%AF%E5%8A%A8/" class="article-date">
  <time datetime="2018-06-23T00:37:17.000Z" itemprop="datePublished">2018-06-23</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/06/23/Flink%E5%9C%A8Mac-OS-X%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E5%90%AF%E5%8A%A8/">Flink在Mac OS X上的安装与启动</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>安装查看java的版本号，推荐使用java8。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> java -version</span></span><br><span class="line">java version "1.8.0_161"</span><br></pre></td></tr></table></figure>
<h3 id="安装Flink"><a href="#安装Flink" class="headerlink" title="安装Flink"></a>安装Flink</h3><p>在Mac OS X上安装Flink是非常方便的。推荐通过homebrew来安装。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> brew install apache-flink</span></span><br><span class="line">...</span><br><span class="line"><span class="meta">$</span><span class="bash"> flink --version</span></span><br><span class="line">Version: 1.5.0, Commit ID: c61b108</span><br></pre></td></tr></table></figure>

<h2 id="启动Flink和进入web交互页面"><a href="#启动Flink和进入web交互页面" class="headerlink" title="启动Flink和进入web交互页面"></a>启动Flink和进入web交互页面</h2><p>Flink可以通过自带的脚本快速启动。首先通过brew来定位刚才安装的Flink的安装目录。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> brew info apache-flink</span></span><br><span class="line">/usr/local/Cellar/apache-flink/1.5.0 (116 files, 324MB) *</span><br><span class="line">  Built from source on 2018-06-22 at 16:42:50</span><br></pre></td></tr></table></figure>
<p>找到之后，进入”1.5.0”目录，然后执行启动脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ./libexec/bin/start-cluster.sh</span></span><br></pre></td></tr></table></figure>
<p>接着就可以进入web页面(<a href="http://localhost:8081/" target="_blank" rel="noopener">http://localhost:8081/</a>)</p>
<h2 id="简单的示例"><a href="#简单的示例" class="headerlink" title="简单的示例"></a>简单的示例</h2><p>首先用IDEA创建一个maven工程。</p>
<p><img src="idea.png" alt="maven工程构件"></p>
<p>创建一个SocketTextStreamWordCount文件，加入以下代码。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SocketTextStreamWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//参数检查</span></span><br><span class="line">        <span class="keyword">if</span> (args.length != <span class="number">2</span>) &#123;</span><br><span class="line">            System.err.println(<span class="string">"USAGE:\nSocketTextStreamWordCount &lt;hostname&gt; &lt;port&gt;"</span>);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        String hostName = args[<span class="number">0</span>];</span><br><span class="line">        Integer port = Integer.parseInt(args[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置环境</span></span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取数据</span></span><br><span class="line">        DataStream&lt;String&gt; text = env.socketTextStream(hostName, port);</span><br><span class="line">		</span><br><span class="line">        <span class="comment">//计数</span></span><br><span class="line">        DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; counts = text.flatMap(<span class="keyword">new</span> LineSplitter())</span><br><span class="line">                .keyBy(<span class="number">0</span>)</span><br><span class="line">                .sum(<span class="number">1</span>);</span><br><span class="line">        counts.print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"Java WordCount from SocketTextStream Example"</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">LineSplitter</span> <span class="keyword">implements</span> <span class="title">FlatMapFunction</span>&lt;<span class="title">String</span>, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector)</span> </span>&#123;</span><br><span class="line">            String[] tokens = s.toLowerCase().split(<span class="string">"\\W+"</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (String token: tokens) &#123;</span><br><span class="line">                <span class="keyword">if</span> (token.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                    collector.collect(<span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(token, <span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>接着进入工程目录，使用以下命令打包。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> maven clean package -Dmaven.test.skip=<span class="literal">true</span></span></span><br></pre></td></tr></table></figure>
<p>然后我们开启监听9000端口。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> nc -l 9000</span></span><br></pre></td></tr></table></figure>

<p>最后进入flink安装目录执行以下命令跑程序。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> /bin/flink run -c org.myorg.quickstart.SocketTextStreamWordCount /your/path/target/original-flink-quickstart-java-1.0-SNAPSHOT.jar 127.0.0.1 9000</span></span><br></pre></td></tr></table></figure>

<p>执行完上述命令后，我们可以在webUI中看到正在运行的程序。</p>
<p><img src="web.png" alt="webUI"></p>
<p>我们可以在nc中输入text，比如</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> nc -l 9000</span></span><br><span class="line">hello hello hello</span><br><span class="line">hehe</span><br><span class="line">your world</span><br></pre></td></tr></table></figure>

<p>然后我们通过tail命令看一下输出的log文件，来观察统计结果。进入flink目录，执行以下命令。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tail -f ./libexec/log/flink-*-taskexecutor-1-localhost.out</span><br><span class="line">(hello,1)</span><br><span class="line">(hello,2)</span><br><span class="line">(hello,3)</span><br><span class="line">(hehe,1)</span><br><span class="line">(your,1)</span><br><span class="line">(world,1)</span><br></pre></td></tr></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文描述了如何在Mac电脑上安装flink，及运行它。接着通过一个简单的flink程序来介绍如何构建及运行flink程序。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://streambench.wordpress.com/2017/10/26/setting-up-apache-flink-on-mac-os-x/" target="_blank" rel="noopener">Setting up Apache Flink on Mac OS X</a><br><a href="http://doc.flink-china.org/1.2.0/index.html" target="_blank" rel="noopener">Apache Flink 中文文档</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/06/23/Flink%E5%9C%A8Mac-OS-X%E4%B8%8A%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E5%90%AF%E5%8A%A8/" data-id="ck736a2qb000jees64g6dg84y" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/flink/" rel="tag">flink</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-scrapy初探" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/06/14/scrapy%E5%88%9D%E6%8E%A2/" class="article-date">
  <time datetime="2018-06-14T07:51:45.000Z" itemprop="datePublished">2018-06-14</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/06/14/scrapy%E5%88%9D%E6%8E%A2/">scrapy初探</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="scrapy"><a href="#scrapy" class="headerlink" title="scrapy"></a>scrapy</h2><p>scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。</p>
<h3 id="安装scrapy"><a href="#安装scrapy" class="headerlink" title="安装scrapy"></a>安装scrapy</h3><p>scrapy支持Python2.7和Python3.4。你可以通过pip安装scrapy。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy</span><br></pre></td></tr></table></figure>

<h3 id="创建scrapy应用"><a href="#创建scrapy应用" class="headerlink" title="创建scrapy应用"></a>创建scrapy应用</h3><p>创建scrapy应用非常简单。首先进入到你想创建scrapy工程的目录，然后在终端执行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject your-project-name</span><br></pre></td></tr></table></figure>
<p>然后它会创建如下目录及文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">your-project-name/</span><br><span class="line">    scrapy.cfg            <span class="comment"># 配置文件</span></span><br><span class="line"></span><br><span class="line">    your-project-name/    <span class="comment"># python模块</span></span><br><span class="line">        __init__.py</span><br><span class="line"></span><br><span class="line">        items.py          <span class="comment"># model定义文件</span></span><br><span class="line"></span><br><span class="line">        middlewares.py    <span class="comment"># 中间件文件</span></span><br><span class="line"></span><br><span class="line">        pipelines.py      <span class="comment"># 管线文件</span></span><br><span class="line"></span><br><span class="line">        settings.py       <span class="comment"># 设置文件</span></span><br><span class="line"></span><br><span class="line">        spiders/          <span class="comment"># 防止爬虫文件的目录</span></span><br><span class="line">            __init__.py</span><br></pre></td></tr></table></figure>
<p>接着你进入到你创建的工程目录中执行以下命令来创建爬虫文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider xxx xxx.xxx.com   <span class="comment">#第一个参数为爬虫文件的名称，第二个参数为需要爬取的域名</span></span><br></pre></td></tr></table></figure>

<p>这样，基本的框架就搭好了。</p>
<h2 id="scrapy架构"><a href="#scrapy架构" class="headerlink" title="scrapy架构"></a>scrapy架构</h2><p>下图展示了scrapy的架构，包括组件及在系统中发生的数据流转。</p>
<p><img src="scrapy_architecture_01.png" alt="架构"></p>
<p>1.引擎从spider中获取初始的爬取请求；<br>2.引擎把这个请求放入到Scheduler组件的队列中，并且去获取下一个爬取请求；<br>3.Scheduler从队列中获得下一个请求返回给引擎；<br>4.引擎通过Downloader中间件把请求发送给Downloader组件；<br>5.一旦页面完成下载，Downloader产生一个响应并通过Downloader中间件发送给引擎；<br>6.引擎收到来自Downloader的响应，通过Spider中间件发送给spider；<br>7.spider处理这个响应并返回item，接着通过Spider中间件发送下一个爬取请求给引擎；<br>8.引擎发送处理过的item给Item Pipelines，接着向Scheduler索取下一个请求；<br>9.程序不停的从步骤一开始重复，直到Scheduler中没有请求为止。</p>
<h2 id="scrapy例子"><a href="#scrapy例子" class="headerlink" title="scrapy例子"></a>scrapy例子</h2><p>以安居客的二手房为例子，我们来做一个爬虫demo。<br>1.创建一个scrapy工程。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject Anjuke</span><br></pre></td></tr></table></figure>
<p>2.进入工程创建爬虫文件。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider anjuke hangzhou.anjuke.com</span><br></pre></td></tr></table></figure>
<p>3.创建一个初始request请求。打开anjuke.py，加入以下代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">    url = <span class="string">'https://hangzhou.anjuke.com/sale/'</span></span><br><span class="line">    <span class="keyword">yield</span> scrapy.Request (url=url, headers=self.headers)</span><br></pre></td></tr></table></figure>
<p>4.定义item。打开items.py，定义需要解析的字段(item相当于model)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">name = scrapy.Field() <span class="comment">##小区</span></span><br><span class="line">address = scrapy.Field() <span class="comment">##位置</span></span><br><span class="line">totalPrice = scrapy.Field() <span class="comment">##总价</span></span><br><span class="line">unitPrice = scrapy.Field() <span class="comment">##单价</span></span><br><span class="line">size = scrapy.Field() <span class="comment">##大小</span></span><br><span class="line">floor = scrapy.Field() <span class="comment">##楼层</span></span><br><span class="line">roomNum = scrapy.Field() <span class="comment">##房间数</span></span><br><span class="line">buildTime = scrapy.Field() <span class="comment">##建造时间</span></span><br><span class="line">publisher = scrapy.Field() <span class="comment">##发布人</span></span><br></pre></td></tr></table></figure>

<p>5.解析item。回到anjuke.py，解析返回的reponse，并赋值给item。xpath是一种用来确定XML文档中某部分位置的语言。可以通过chrome的开发者工具中的copy xpath快速给出所需信息的路径。这边解析了前面50个页面。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">page = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">       houseList = response.xpath (<span class="string">'//*[@id="houselist-mod-new"]/li'</span>)</span><br><span class="line">       <span class="keyword">for</span> div <span class="keyword">in</span> houseList:</span><br><span class="line">           item = &#123;&#125;</span><br><span class="line">           houseDetails = div.xpath (<span class="string">'./div[@class="house-details"]'</span>)</span><br><span class="line">           commAddress = houseDetails.xpath (<span class="string">'./div[3]/span/@title'</span>).extract_first ()</span><br><span class="line"></span><br><span class="line">           item[<span class="string">'name'</span>] = commAddress.split ()[<span class="number">0</span>]</span><br><span class="line">           item[<span class="string">'address'</span>] = commAddress.split ()[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">           item[<span class="string">'roomNum'</span>] = houseDetails.xpath(<span class="string">'./div[2]/span[1]/text()'</span>).extract_first()</span><br><span class="line">           item[<span class="string">'size'</span>] = houseDetails.xpath(<span class="string">'./div[2]/span[2]/text()'</span>).extract_first()</span><br><span class="line">           item[<span class="string">'floor'</span>] = houseDetails.xpath (<span class="string">'./div[2]/span[3]/text()'</span>).extract_first ()</span><br><span class="line">           item[<span class="string">'builtTime'</span>] = houseDetails.xpath (<span class="string">'./div[2]/span[4]/text()'</span>).extract_first ()</span><br><span class="line">           item[<span class="string">'publisher'</span>] = houseDetails.xpath (<span class="string">'./div[2]/span[5]/text()'</span>).extract_first ()</span><br><span class="line"></span><br><span class="line">           proPrice = div.xpath(<span class="string">'./div[@class="pro-price"]'</span>)</span><br><span class="line">           item[<span class="string">'totalPrice'</span>] = proPrice.xpath(<span class="string">'./span[@class="price-det"]/strong/text()'</span>).extract_first() +\</span><br><span class="line">                                proPrice.xpath(<span class="string">'./span[@class="price-det"]/text()'</span>).extract_first()</span><br><span class="line">           item[<span class="string">'unitPrice'</span>] = proPrice.xpath(<span class="string">'./span[@class="unit-price"]/text()'</span>).extract_first()</span><br><span class="line">           <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">       <span class="keyword">if</span> self.page &lt;= <span class="number">50</span>:</span><br><span class="line">           self.page += <span class="number">1</span></span><br><span class="line">           next_url = response.xpath(<span class="string">'//div[@class="multi-page"]/a[last()]/@href'</span>).extract_first()</span><br><span class="line"></span><br><span class="line">           <span class="keyword">yield</span> scrapy.Request(url=next_url,callback=self.parse,dont_filter=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>6.存储item。这边使用了mysql来存储爬取的数据。打开pipelines.py。首先在open_spider函数中打开数据库，删除原先的表，并新建一张houseinfo表。接着在process_item函数中处理一下item，并把数据插入到表中，最后返回item。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line"></span><br><span class="line">    self.conn = pymysql.connect(</span><br><span class="line">        host=<span class="string">"127.0.0.1"</span>,</span><br><span class="line">        user=<span class="string">"root"</span>,</span><br><span class="line">        password=<span class="string">""</span>,</span><br><span class="line">        database=<span class="string">"anjuke"</span>,</span><br><span class="line">        charset=<span class="string">'utf8'</span>,</span><br><span class="line">        cursorclass=pymysql.cursors.DictCursor )</span><br><span class="line">    self.cursor = self.conn.cursor()</span><br><span class="line">    self.cursor.execute(<span class="string">"drop table if exists houseinfo"</span>)</span><br><span class="line"></span><br><span class="line">    createsql = <span class="string">"""create table houseinfo(name VARCHAR(32) NOT NULL,</span></span><br><span class="line"><span class="string">              address VARCHAR(32) NOT NULL,</span></span><br><span class="line"><span class="string">              totalPrice VARCHAR(32) NOT NULL,</span></span><br><span class="line"><span class="string">              unitPrice VARCHAR(32) NOT NULL,</span></span><br><span class="line"><span class="string">              size VARCHAR(32) NOT NULL,</span></span><br><span class="line"><span class="string">              floor VARCHAR(32) NOT NULL,</span></span><br><span class="line"><span class="string">              roomNum VARCHAR(32) NOT NULL,</span></span><br><span class="line"><span class="string">              buildTime VARCHAR(32) NOT NULL,</span></span><br><span class="line"><span class="string">              publisher VARCHAR(32) NOT NULL,</span></span><br><span class="line"><span class="string">              area varchar(8) not null)"""</span></span><br><span class="line">    self.cursor.execute(createsql)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">    self.cursor.close()</span><br><span class="line">    self.conn.close()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">    item[<span class="string">'totalPrice'</span>] = item[<span class="string">'totalPrice'</span>].split(<span class="string">'万'</span>)[<span class="number">0</span>]</span><br><span class="line">    item[<span class="string">'unitPrice'</span>] = item[<span class="string">'unitPrice'</span>].split(<span class="string">'元'</span>)[<span class="number">0</span>]</span><br><span class="line">    item[<span class="string">'builtTime'</span>] = item[<span class="string">'builtTime'</span>].split(<span class="string">'年'</span>)[<span class="number">0</span>]</span><br><span class="line">    item[<span class="string">'size'</span>] = item[<span class="string">'size'</span>].split(<span class="string">'m'</span>)[<span class="number">0</span>]</span><br><span class="line">    area = item[<span class="string">'address'</span>].split(<span class="string">'-'</span>)[<span class="number">0</span>]</span><br><span class="line">    insertsql = <span class="string">'insert into houseinfo(name, address,totalPrice, unitPrice, size, floor, roomNum, \</span></span><br><span class="line"><span class="string">                    buildTime, publisher,area) \</span></span><br><span class="line"><span class="string">                    VALUES ("%s", "%s","%s", "%s","%s", "%s","%s", "%s","%s","%s")'</span> % \</span><br><span class="line">                    (item[<span class="string">'name'</span>], item[<span class="string">'address'</span>], item[<span class="string">'totalPrice'</span>], item[<span class="string">'unitPrice'</span>], item[<span class="string">'size'</span>], item[<span class="string">'floor'</span>], item[<span class="string">'roomNum'</span>], item[<span class="string">'builtTime'</span>], item[<span class="string">'publisher'</span>],area)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        self.cursor.execute(insertsql)</span><br><span class="line">        self.conn.commit()</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        self.conn.rollback()</span><br><span class="line">        print(e)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>

<p>7.最后设置一下爬取的策略及配置。打开settings.py。这边设置了user_agent，robotstxt_obey，download_delay等参数来进行初步的反扒策略。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span></span><br><span class="line">USER_AGENT = <span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Obey robots.txt rules</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># See also autothrottle settings and docs</span></span><br><span class="line">DOWNLOAD_DELAY = <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Disable cookies (enabled by default)</span></span><br><span class="line">COOKIES_ENABLED = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Override the default request headers:</span></span><br><span class="line">DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">  <span class="string">'Accept'</span>: <span class="string">'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'</span>,</span><br><span class="line">  <span class="string">'Accept-Language'</span>: <span class="string">'en'</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># See https://doc.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'Anjuke.pipelines.AnjukePipeline'</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>这样，一个简单的爬虫程序就完成了。详细的代码可以查看<a href="https://github.com/bigjar/Anjuke.git。" target="_blank" rel="noopener">https://github.com/bigjar/Anjuke.git。</a></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>1.<a href="https://docs.scrapy.org/en/latest/" target="_blank" rel="noopener">Scrapy 1.5 documentation</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/06/14/scrapy%E5%88%9D%E6%8E%A2/" data-id="ck736a2qz0028ees6f45h0r8m" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/scrapy/" rel="tag">scrapy</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%88%AC%E8%99%AB/" rel="tag">爬虫</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/">Next &amp;raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/iOS/">iOS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AE%89%E5%85%A8/">安全</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/">微服务</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Actuator/" rel="tag">Actuator</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Flink/" rel="tag">Flink</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Flume/" rel="tag">Flume</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Grafana/" rel="tag">Grafana</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/KVO/" rel="tag">KVO</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kafka/" rel="tag">Kafka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Objective-C/" rel="tag">Objective-C</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Prometheus/" rel="tag">Prometheus</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spring-Boot/" rel="tag">Spring Boot</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Swift/" rel="tag">Swift</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/akka/" rel="tag">akka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/block/" rel="tag">block</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flink/" rel="tag">flink</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kafka/" rel="tag">kafka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/runloop/" rel="tag">runloop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/runtime/" rel="tag">runtime</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scrapy/" rel="tag">scrapy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%87%BD%E6%95%B0%E6%B4%BE%E5%8F%91/" rel="tag">函数派发</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%93%8D%E5%BA%94%E8%BF%9E/" rel="tag">响应连</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%89%E5%85%A8/" rel="tag">安全</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AF%86%E9%92%A5/" rel="tag">密钥</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/" rel="tag">源码分析</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%88%AC%E8%99%AB/" rel="tag">爬虫</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%94%81/" rel="tag">锁</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Actuator/" style="font-size: 12.5px;">Actuator</a> <a href="/tags/Flink/" style="font-size: 12.5px;">Flink</a> <a href="/tags/Flume/" style="font-size: 10px;">Flume</a> <a href="/tags/Grafana/" style="font-size: 10px;">Grafana</a> <a href="/tags/Hadoop/" style="font-size: 12.5px;">Hadoop</a> <a href="/tags/KVO/" style="font-size: 10px;">KVO</a> <a href="/tags/Kafka/" style="font-size: 10px;">Kafka</a> <a href="/tags/Objective-C/" style="font-size: 20px;">Objective-C</a> <a href="/tags/Prometheus/" style="font-size: 10px;">Prometheus</a> <a href="/tags/Spark/" style="font-size: 10px;">Spark</a> <a href="/tags/Spring-Boot/" style="font-size: 12.5px;">Spring Boot</a> <a href="/tags/Swift/" style="font-size: 12.5px;">Swift</a> <a href="/tags/akka/" style="font-size: 10px;">akka</a> <a href="/tags/block/" style="font-size: 10px;">block</a> <a href="/tags/flink/" style="font-size: 15px;">flink</a> <a href="/tags/kafka/" style="font-size: 12.5px;">kafka</a> <a href="/tags/runloop/" style="font-size: 10px;">runloop</a> <a href="/tags/runtime/" style="font-size: 10px;">runtime</a> <a href="/tags/scrapy/" style="font-size: 10px;">scrapy</a> <a href="/tags/%E5%87%BD%E6%95%B0%E6%B4%BE%E5%8F%91/" style="font-size: 10px;">函数派发</a> <a href="/tags/%E5%93%8D%E5%BA%94%E8%BF%9E/" style="font-size: 10px;">响应连</a> <a href="/tags/%E5%AE%89%E5%85%A8/" style="font-size: 10px;">安全</a> <a href="/tags/%E5%AF%86%E9%92%A5/" style="font-size: 10px;">密钥</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 12.5px;">机器学习</a> <a href="/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/" style="font-size: 17.5px;">源码分析</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 10px;">爬虫</a> <a href="/tags/%E9%94%81/" style="font-size: 10px;">锁</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/02/20/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Flink%E7%BD%91%E7%BB%9C%E6%A0%88%E5%8F%8A%E8%83%8C%E5%8E%8B%E7%9B%91%E6%8E%A7/">深入理解Flink网络栈及背压监控</a>
          </li>
        
          <li>
            <a href="/2019/11/08/%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97Flink%E9%9B%86%E7%BE%A4%E8%A7%84%E6%A8%A1%EF%BC%9A%E4%BF%A1%E5%B0%81%E8%83%8C%E8%AE%A1%E7%AE%97%E6%B3%95/">如何计算Flink集群规模：信封背计算法</a>
          </li>
        
          <li>
            <a href="/2018/12/22/%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8Hadoop%E9%9B%86%E7%BE%A4/">如何搭建高可用Hadoop集群</a>
          </li>
        
          <li>
            <a href="/2018/11/13/Akka%E5%9C%A8Flink%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/">Akka在Flink中的应用</a>
          </li>
        
          <li>
            <a href="/2018/08/19/Spring-Boot-Metrics%E7%9B%91%E6%8E%A7%E4%B9%8BPrometheus-Grafana/">Spring Boot Metrics监控之Prometheus&amp;Grafana</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 MrHup<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>